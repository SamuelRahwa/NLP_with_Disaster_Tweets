{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd091f96",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-05T19:18:43.370151Z",
     "start_time": "2022-07-05T19:18:43.365544Z"
    }
   },
   "outputs": [],
   "source": [
    "# Bag Of Words:\n",
    "\n",
    "# We define a fixed length vector where each entry corresponds to a word in our pre-defined dictionary of words. The size of the vector equals the size of the dictionary. \n",
    "# Then, for representing a text using this vector, we count how many times each word of our dictionary appears in the text and we put this number in the corresponding vector entry.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Tf-IDF:\n",
    "\n",
    "# Term frequency-inverse document frequency (TF-IDF) gives a measure that takes the importance of a word into consideration depending on how frequently it occurs in a document and a corpus.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Word to Vector:\n",
    "\n",
    "# Converting words to vectors, or word vectorization, is a natural language processing (NLP) process. \n",
    "# The process uses language models to map words into vector space. A vector space represents each word by a vector of real numbers. It also allows words with similar meanings have similar representations\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Count Vectorizer: \n",
    "\n",
    "# It will fit and learn the word vocabulary and try to create a document term matrix in which the individual cells denote the frequency of that word in a particular document, which is also known as term frequency, and the columns are dedicated to each word in the corpus.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Transfer Learning:\n",
    "\n",
    "# It uses a prebuilt model on your data and gives pretty good results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d5483f46",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-05T20:28:23.326689Z",
     "start_time": "2022-07-05T20:28:23.323258Z"
    }
   },
   "outputs": [],
   "source": [
    "# https://medium.com/analytics-vidhya/nlp-tutorial-for-text-classification-in-python-8f19cd17b49e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "9960b56d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-05T22:54:42.914064Z",
     "start_time": "2022-07-05T22:54:42.733086Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/sammy/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/sammy/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to /Users/sammy/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# For data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "# For Plotting\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "plt.style.use('ggplot')\n",
    "from matplotlib.pyplot import figure\n",
    "matplotlib.rcParams['figure.figsize'] = (22,10)\n",
    "plt.rcParams.update({'font.size': 18})\n",
    "\n",
    "import seaborn as sns \n",
    "sns.set_style('darkgrid')\n",
    "\n",
    "# For processing\n",
    "import re, string\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from sklearn import feature_extraction, model_selection, naive_bayes, pipeline, manifold, preprocessing, feature_selection\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.metrics import classification_report, f1_score, accuracy_score, confusion_matrix\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
    "\n",
    "\n",
    "# For explainer\n",
    "from lime import lime_text\n",
    "\n",
    "# For word embedding\n",
    "import gensim\n",
    "import gensim.downloader as gensim_api\n",
    "\n",
    "# For deep learning\n",
    "from tensorflow.keras import models, layers, preprocessing as kprocessing\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "# For bert language model\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "e3a2f84b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-05T22:54:43.489116Z",
     "start_time": "2022-07-05T22:54:43.424530Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7503 entries, 0 to 7502\n",
      "Data columns (total 16 columns):\n",
      " #   Column                     Non-Null Count  Dtype  \n",
      "---  ------                     --------------  -----  \n",
      " 0   keyword                    7447 non-null   object \n",
      " 1   location                   5021 non-null   object \n",
      " 2   text                       7503 non-null   object \n",
      " 3   target                     7503 non-null   int64  \n",
      " 4   total_words                7503 non-null   int64  \n",
      " 5   char_count                 7503 non-null   int64  \n",
      " 6   sentence_count             7503 non-null   int64  \n",
      " 7   avg_word_length            7503 non-null   float64\n",
      " 8   avg_sentence_lenght        7503 non-null   float64\n",
      " 9   tokenized_text             7503 non-null   object \n",
      " 10  clean_text                 7460 non-null   object \n",
      " 11  clean_total_words          7503 non-null   int64  \n",
      " 12  clean_char_count           7503 non-null   int64  \n",
      " 13  clean_sentence_count       7503 non-null   int64  \n",
      " 14  clean_avg_word_length      7503 non-null   float64\n",
      " 15  clean_avg_sentence_lenght  7503 non-null   float64\n",
      "dtypes: float64(4), int64(7), object(5)\n",
      "memory usage: 938.0+ KB\n"
     ]
    }
   ],
   "source": [
    "# importing dataset\n",
    "\n",
    "data = pd.read_csv('preprocessed_clean_train_set.csv') \n",
    "\n",
    "\n",
    "# Reviewing the data shape, columns and data types\n",
    "\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "877cbb3a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-05T22:54:45.576022Z",
     "start_time": "2022-07-05T22:54:45.555887Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Total</th>\n",
       "      <th>Missing Percent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>location</th>\n",
       "      <td>2482</td>\n",
       "      <td>33.080101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>keyword</th>\n",
       "      <td>56</td>\n",
       "      <td>0.746368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>clean_text</th>\n",
       "      <td>43</td>\n",
       "      <td>0.573104</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Total  Missing Percent\n",
       "location     2482        33.080101\n",
       "keyword        56         0.746368\n",
       "clean_text     43         0.573104"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Percentage of Data Missing Per Column Above 1%\n",
    "\n",
    "total = data.isnull().sum().sort_values(ascending=False)\n",
    "percent = (data.isnull().sum()/data.isnull().count()).sort_values(ascending=False)\n",
    "missing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Missing Percent'])\n",
    "missing_data['Missing Percent'] = missing_data['Missing Percent'].apply(lambda x: x * 100)\n",
    "missing_data.loc[missing_data['Missing Percent'] > .01][:152]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "719047a6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-05T22:54:47.236600Z",
     "start_time": "2022-07-05T22:54:47.211620Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>total_words</th>\n",
       "      <th>char_count</th>\n",
       "      <th>sentence_count</th>\n",
       "      <th>avg_word_length</th>\n",
       "      <th>avg_sentence_lenght</th>\n",
       "      <th>tokenized_text</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>clean_total_words</th>\n",
       "      <th>clean_char_count</th>\n",
       "      <th>clean_sentence_count</th>\n",
       "      <th>clean_avg_word_length</th>\n",
       "      <th>clean_avg_sentence_lenght</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>this is ridiculous....</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>20</td>\n",
       "      <td>5</td>\n",
       "      <td>6.666667</td>\n",
       "      <td>0.6</td>\n",
       "      <td>['this', 'is', 'ridiculous', '...']</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LOOOOOOL</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>['LOOOOOOL']</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>accident</td>\n",
       "      <td>NaN</td>\n",
       "      <td>???? it was an accident http://t.co/Oia5fxi4gM</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>41</td>\n",
       "      <td>2</td>\n",
       "      <td>6.833333</td>\n",
       "      <td>3.0</td>\n",
       "      <td>['?', '?', '?', 'it', 'was', 'an', 'accident',...</td>\n",
       "      <td>accident</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>accident</td>\n",
       "      <td>Alberta | Sask. | Montana</td>\n",
       "      <td>Suffield Alberta Accident https://t.co/bPTmlF4P10</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>46</td>\n",
       "      <td>2</td>\n",
       "      <td>11.500000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>['Suffield', 'Alberta', 'Accident', 'https://t...</td>\n",
       "      <td>accident</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>accident</td>\n",
       "      <td>Gloucestershire , UK</td>\n",
       "      <td>@flowri were you marinading it or was it an ac...</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>44</td>\n",
       "      <td>1</td>\n",
       "      <td>4.400000</td>\n",
       "      <td>10.0</td>\n",
       "      <td>['@flowri', 'were', 'you', 'marinading', 'it',...</td>\n",
       "      <td>accident</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7492</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>#??? #?? #??? #??? MH370: Aircraft debris foun...</td>\n",
       "      <td>1</td>\n",
       "      <td>18</td>\n",
       "      <td>105</td>\n",
       "      <td>5</td>\n",
       "      <td>5.833333</td>\n",
       "      <td>3.6</td>\n",
       "      <td>['#', '?', '?', '?', '#', '?', '?', '#', '?', ...</td>\n",
       "      <td>aircraft debris find la reunion miss malaysia ...</td>\n",
       "      <td>8</td>\n",
       "      <td>47</td>\n",
       "      <td>1</td>\n",
       "      <td>5.875000</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7495</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>#breaking #LA Refugio oil spill may have been ...</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>87</td>\n",
       "      <td>2</td>\n",
       "      <td>6.692308</td>\n",
       "      <td>6.5</td>\n",
       "      <td>['#breaking', '#LA', 'Refugio', 'oil', 'spill'...</td>\n",
       "      <td>refugio oil spill costlier big project</td>\n",
       "      <td>6</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "      <td>5.500000</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7498</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>#WorldNews Fallen powerlines on G:link tram: U...</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>118</td>\n",
       "      <td>5</td>\n",
       "      <td>6.210526</td>\n",
       "      <td>3.8</td>\n",
       "      <td>['#WorldNews', 'Fallen', 'powerlines', 'on', '...</td>\n",
       "      <td>fall pipeline link tram update fire crew evacu...</td>\n",
       "      <td>9</td>\n",
       "      <td>51</td>\n",
       "      <td>1</td>\n",
       "      <td>5.666667</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7501</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Two giant cranes holding a bridge collapse int...</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>73</td>\n",
       "      <td>2</td>\n",
       "      <td>6.636364</td>\n",
       "      <td>5.5</td>\n",
       "      <td>['Two', 'giant', 'cranes', 'holding', 'a', 'br...</td>\n",
       "      <td>giant crane hold bridge collapse nearby home</td>\n",
       "      <td>7</td>\n",
       "      <td>38</td>\n",
       "      <td>1</td>\n",
       "      <td>5.428571</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7502</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The Latest: More Homes Razed by Northern Calif...</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>82</td>\n",
       "      <td>2</td>\n",
       "      <td>6.307692</td>\n",
       "      <td>6.5</td>\n",
       "      <td>['The', 'Latest', ':', 'More', 'Homes', 'Razed...</td>\n",
       "      <td>late home raze northern california wildfire ac...</td>\n",
       "      <td>8</td>\n",
       "      <td>45</td>\n",
       "      <td>1</td>\n",
       "      <td>5.625000</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1075 rows × 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       keyword                   location  \\\n",
       "20         NaN                        NaN   \n",
       "24         NaN                        NaN   \n",
       "89    accident                        NaN   \n",
       "93    accident  Alberta | Sask. | Montana   \n",
       "97    accident       Gloucestershire , UK   \n",
       "...        ...                        ...   \n",
       "7492       NaN                        NaN   \n",
       "7495       NaN                        NaN   \n",
       "7498       NaN                        NaN   \n",
       "7501       NaN                        NaN   \n",
       "7502       NaN                        NaN   \n",
       "\n",
       "                                                   text  target  total_words  \\\n",
       "20                               this is ridiculous....       0            3   \n",
       "24                                             LOOOOOOL       0            1   \n",
       "89       ???? it was an accident http://t.co/Oia5fxi4gM       0            6   \n",
       "93    Suffield Alberta Accident https://t.co/bPTmlF4P10       1            4   \n",
       "97    @flowri were you marinading it or was it an ac...       0           10   \n",
       "...                                                 ...     ...          ...   \n",
       "7492  #??? #?? #??? #??? MH370: Aircraft debris foun...       1           18   \n",
       "7495  #breaking #LA Refugio oil spill may have been ...       1           13   \n",
       "7498  #WorldNews Fallen powerlines on G:link tram: U...       1           19   \n",
       "7501  Two giant cranes holding a bridge collapse int...       1           11   \n",
       "7502  The Latest: More Homes Razed by Northern Calif...       1           13   \n",
       "\n",
       "      char_count  sentence_count  avg_word_length  avg_sentence_lenght  \\\n",
       "20            20               5         6.666667                  0.6   \n",
       "24             8               1         8.000000                  1.0   \n",
       "89            41               2         6.833333                  3.0   \n",
       "93            46               2        11.500000                  2.0   \n",
       "97            44               1         4.400000                 10.0   \n",
       "...          ...             ...              ...                  ...   \n",
       "7492         105               5         5.833333                  3.6   \n",
       "7495          87               2         6.692308                  6.5   \n",
       "7498         118               5         6.210526                  3.8   \n",
       "7501          73               2         6.636364                  5.5   \n",
       "7502          82               2         6.307692                  6.5   \n",
       "\n",
       "                                         tokenized_text  \\\n",
       "20                  ['this', 'is', 'ridiculous', '...']   \n",
       "24                                         ['LOOOOOOL']   \n",
       "89    ['?', '?', '?', 'it', 'was', 'an', 'accident',...   \n",
       "93    ['Suffield', 'Alberta', 'Accident', 'https://t...   \n",
       "97    ['@flowri', 'were', 'you', 'marinading', 'it',...   \n",
       "...                                                 ...   \n",
       "7492  ['#', '?', '?', '?', '#', '?', '?', '#', '?', ...   \n",
       "7495  ['#breaking', '#LA', 'Refugio', 'oil', 'spill'...   \n",
       "7498  ['#WorldNews', 'Fallen', 'powerlines', 'on', '...   \n",
       "7501  ['Two', 'giant', 'cranes', 'holding', 'a', 'br...   \n",
       "7502  ['The', 'Latest', ':', 'More', 'Homes', 'Razed...   \n",
       "\n",
       "                                             clean_text  clean_total_words  \\\n",
       "20                                                  NaN                  1   \n",
       "24                                                  NaN                  1   \n",
       "89                                             accident                  1   \n",
       "93                                             accident                  1   \n",
       "97                                             accident                  1   \n",
       "...                                                 ...                ...   \n",
       "7492  aircraft debris find la reunion miss malaysia ...                  8   \n",
       "7495             refugio oil spill costlier big project                  6   \n",
       "7498  fall pipeline link tram update fire crew evacu...                  9   \n",
       "7501       giant crane hold bridge collapse nearby home                  7   \n",
       "7502  late home raze northern california wildfire ac...                  8   \n",
       "\n",
       "      clean_char_count  clean_sentence_count  clean_avg_word_length  \\\n",
       "20                   0                     1               0.000000   \n",
       "24                   0                     1               0.000000   \n",
       "89                   8                     1               8.000000   \n",
       "93                   8                     1               8.000000   \n",
       "97                   8                     1               8.000000   \n",
       "...                ...                   ...                    ...   \n",
       "7492                47                     1               5.875000   \n",
       "7495                33                     1               5.500000   \n",
       "7498                51                     1               5.666667   \n",
       "7501                38                     1               5.428571   \n",
       "7502                45                     1               5.625000   \n",
       "\n",
       "      clean_avg_sentence_lenght  \n",
       "20                          1.0  \n",
       "24                          1.0  \n",
       "89                          1.0  \n",
       "93                          1.0  \n",
       "97                          1.0  \n",
       "...                         ...  \n",
       "7492                        8.0  \n",
       "7495                        6.0  \n",
       "7498                        9.0  \n",
       "7501                        7.0  \n",
       "7502                        8.0  \n",
       "\n",
       "[1075 rows x 16 columns]"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking for Duplicates \n",
    "\n",
    "data[data.duplicated(subset = \"clean_text\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "d95b132f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-05T22:54:49.687713Z",
     "start_time": "2022-07-05T22:54:49.680535Z"
    }
   },
   "outputs": [],
   "source": [
    "# Dropping Rows with any missing values\n",
    "\n",
    "data.dropna(axis = 0, subset = ['clean_text'], inplace = True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "34c8efee",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-05T22:54:50.374129Z",
     "start_time": "2022-07-05T22:54:50.351823Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Total</th>\n",
       "      <th>Missing Percent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>location</th>\n",
       "      <td>2458</td>\n",
       "      <td>32.949062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>keyword</th>\n",
       "      <td>53</td>\n",
       "      <td>0.710456</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Total  Missing Percent\n",
       "location   2458        32.949062\n",
       "keyword      53         0.710456"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Percentage of Data Missing Per Column Above 1%\n",
    "\n",
    "total = data.isnull().sum().sort_values(ascending=False)\n",
    "percent = (data.isnull().sum()/data.isnull().count()).sort_values(ascending=False)\n",
    "missing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Missing Percent'])\n",
    "missing_data['Missing Percent'] = missing_data['Missing Percent'].apply(lambda x: x * 100)\n",
    "missing_data.loc[missing_data['Missing Percent'] > .01][:152]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "79b973e3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-05T22:54:52.098730Z",
     "start_time": "2022-07-05T22:54:52.093494Z"
    }
   },
   "outputs": [],
   "source": [
    "train = data[['clean_text', 'target']][0:5595]\n",
    "validation = data[['clean_text', 'target']][5595:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "8e53770a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-05T22:54:52.535762Z",
     "start_time": "2022-07-05T22:54:52.524770Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clean_text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>reason allah forgive</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>forest fire near la range canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>resident ask shelter place officer evacuation ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>people evacuation order california</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>get send photo smoke school</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5625</th>\n",
       "      <td>video pick body water rescuer search hundred m...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5626</th>\n",
       "      <td>fear miss migrant med rescuer search survivor ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5627</th>\n",
       "      <td>rescuer search hundred migrant mediterranean boat</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5628</th>\n",
       "      <td>desire watch rescuer</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5629</th>\n",
       "      <td>fear miss migrant med rescuer search survivor ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5595 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             clean_text  target\n",
       "0                                  reason allah forgive       1\n",
       "1                      forest fire near la range canada       1\n",
       "2     resident ask shelter place officer evacuation ...       1\n",
       "3                    people evacuation order california       1\n",
       "4                           get send photo smoke school       1\n",
       "...                                                 ...     ...\n",
       "5625  video pick body water rescuer search hundred m...       1\n",
       "5626  fear miss migrant med rescuer search survivor ...       1\n",
       "5627  rescuer search hundred migrant mediterranean boat       1\n",
       "5628                               desire watch rescuer       0\n",
       "5629  fear miss migrant med rescuer search survivor ...       1\n",
       "\n",
       "[5595 rows x 2 columns]"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "4168c928",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-05T22:54:53.057011Z",
     "start_time": "2022-07-05T22:54:53.047930Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clean_text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5630</th>\n",
       "      <td>channel spa dog rescuer door</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5631</th>\n",
       "      <td>video pick body water rescuer search hundred m...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5632</th>\n",
       "      <td>woman gas app guide rescuer injure county</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5633</th>\n",
       "      <td>video pick body water rescuer search hundred m...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5634</th>\n",
       "      <td>video pick body water rescuer search hundred m...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7498</th>\n",
       "      <td>fall pipeline link tram update fire crew evacu...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7499</th>\n",
       "      <td>flip bomb evacuate stay blow</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7500</th>\n",
       "      <td>suicide bomber kill saudi security site mosque...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7501</th>\n",
       "      <td>giant crane hold bridge collapse nearby home</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7502</th>\n",
       "      <td>late home raze northern california wildfire ac...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1865 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             clean_text  target\n",
       "5630                       channel spa dog rescuer door       0\n",
       "5631  video pick body water rescuer search hundred m...       1\n",
       "5632          woman gas app guide rescuer injure county       1\n",
       "5633  video pick body water rescuer search hundred m...       1\n",
       "5634  video pick body water rescuer search hundred m...       1\n",
       "...                                                 ...     ...\n",
       "7498  fall pipeline link tram update fire crew evacu...       1\n",
       "7499                       flip bomb evacuate stay blow       1\n",
       "7500  suicide bomber kill saudi security site mosque...       1\n",
       "7501       giant crane hold bridge collapse nearby home       1\n",
       "7502  late home raze northern california wildfire ac...       1\n",
       "\n",
       "[1865 rows x 2 columns]"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "327e268d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-05T22:45:15.888048Z",
     "start_time": "2022-07-05T22:45:15.882292Z"
    }
   },
   "outputs": [],
   "source": [
    "# # splitting the data into a train and validation\n",
    "\n",
    "# X_train, X_validation, y_train, y_validation = train_test_split(X, \n",
    "#                                                                 y, \n",
    "#                                                                 test_size = 0.25, \n",
    "#                                                                 shuffle = True,\n",
    "#                                                                 random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "8b317499",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-05T22:55:08.230339Z",
     "start_time": "2022-07-05T22:55:08.227083Z"
    }
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras import layers\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Conv2D, MaxPool2D, Dropout, Flatten, BatchNormalization\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from keras import regularizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "ab81818d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-05T22:55:08.837868Z",
     "start_time": "2022-07-05T22:55:08.835315Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow_hub as hub \n",
    "import tensorflow as tf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "0e3d8a25",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-05T22:55:12.571346Z",
     "start_time": "2022-07-05T22:55:10.691094Z"
    }
   },
   "outputs": [],
   "source": [
    "elmo = hub.load(\"https://tfhub.dev/google/elmo/3\").signatures[\"default\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "e5bf808b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-05T22:55:16.196789Z",
     "start_time": "2022-07-05T22:55:16.141528Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(1), Dimension(8), Dimension(1024)])"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# just a random sentence\n",
    "x = [\"Roasted ants are a popular snack in Columbia\"]\n",
    "\n",
    "# Extract ELMo features \n",
    "embeddings = elmo(tf.constant(x))[\"elmo\"]\n",
    "\n",
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "73cbb4c7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-05T23:04:18.527784Z",
     "start_time": "2022-07-05T23:04:18.521753Z"
    }
   },
   "outputs": [],
   "source": [
    "def elmo_vectors(x):\n",
    "    embeddings = elmo(tf.constant(x.to_list()))[\"elmo\"]\n",
    "    \n",
    "    with tf.compat.v1.Session() as sess:\n",
    "        sess.run(tf.compat.v1.global_variables_initializer())\n",
    "        sess.run(tf.compat.v1.tables_initializer())\n",
    "        # return average of ELMo features\n",
    "        return sess.run(tf.reduce_mean(embeddings,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "49e5ef66",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-05T23:04:18.919975Z",
     "start_time": "2022-07-05T23:04:18.910778Z"
    }
   },
   "outputs": [],
   "source": [
    "list_train = [train[i:i+100] for i in range(0,train.shape[0],100)] \n",
    "list_validation = [validation[i:i+100] for i in range(0,validation.shape[0],100)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b7262ac",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-07-05T23:04:23.674Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-05 17:04:26.600636: W tensorflow/core/grappler/costs/op_level_cost_estimator.cc:690] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"CPU\" vendor: \"GenuineIntel\" model: \"110\" frequency: 3100 num_cores: 8 environment { key: \"cpu_instruction_set\" value: \"SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4.2\" } environment { key: \"eigen\" value: \"3.4.90\" } l1_cache_size: 32768 l2_cache_size: 262144 l3_cache_size: 8388608 memory_size: 268435456 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n",
      "2022-07-05 17:04:26.876456: W tensorflow/core/grappler/costs/op_level_cost_estimator.cc:690] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"CPU\" vendor: \"GenuineIntel\" model: \"110\" frequency: 3100 num_cores: 8 environment { key: \"cpu_instruction_set\" value: \"SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4.2\" } environment { key: \"eigen\" value: \"3.4.90\" } l1_cache_size: 32768 l2_cache_size: 262144 l3_cache_size: 8388608 memory_size: 268435456 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n",
      "2022-07-05 17:12:02.157166: W tensorflow/core/grappler/costs/op_level_cost_estimator.cc:690] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"CPU\" vendor: \"GenuineIntel\" model: \"110\" frequency: 3100 num_cores: 8 environment { key: \"cpu_instruction_set\" value: \"SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4.2\" } environment { key: \"eigen\" value: \"3.4.90\" } l1_cache_size: 32768 l2_cache_size: 262144 l3_cache_size: 8388608 memory_size: 268435456 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n",
      "2022-07-05 17:12:02.461487: W tensorflow/core/grappler/costs/op_level_cost_estimator.cc:690] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"CPU\" vendor: \"GenuineIntel\" model: \"110\" frequency: 3100 num_cores: 8 environment { key: \"cpu_instruction_set\" value: \"SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4.2\" } environment { key: \"eigen\" value: \"3.4.90\" } l1_cache_size: 32768 l2_cache_size: 262144 l3_cache_size: 8388608 memory_size: 268435456 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n",
      "2022-07-05 17:19:58.526157: W tensorflow/core/grappler/costs/op_level_cost_estimator.cc:690] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"CPU\" vendor: \"GenuineIntel\" model: \"110\" frequency: 3100 num_cores: 8 environment { key: \"cpu_instruction_set\" value: \"SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4.2\" } environment { key: \"eigen\" value: \"3.4.90\" } l1_cache_size: 32768 l2_cache_size: 262144 l3_cache_size: 8388608 memory_size: 268435456 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n",
      "2022-07-05 17:19:58.842239: W tensorflow/core/grappler/costs/op_level_cost_estimator.cc:690] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"CPU\" vendor: \"GenuineIntel\" model: \"110\" frequency: 3100 num_cores: 8 environment { key: \"cpu_instruction_set\" value: \"SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4.2\" } environment { key: \"eigen\" value: \"3.4.90\" } l1_cache_size: 32768 l2_cache_size: 262144 l3_cache_size: 8388608 memory_size: 268435456 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n",
      "2022-07-05 17:29:19.191709: W tensorflow/core/grappler/costs/op_level_cost_estimator.cc:690] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"CPU\" vendor: \"GenuineIntel\" model: \"110\" frequency: 3100 num_cores: 8 environment { key: \"cpu_instruction_set\" value: \"SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4.2\" } environment { key: \"eigen\" value: \"3.4.90\" } l1_cache_size: 32768 l2_cache_size: 262144 l3_cache_size: 8388608 memory_size: 268435456 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n",
      "2022-07-05 17:29:19.509960: W tensorflow/core/grappler/costs/op_level_cost_estimator.cc:690] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"CPU\" vendor: \"GenuineIntel\" model: \"110\" frequency: 3100 num_cores: 8 environment { key: \"cpu_instruction_set\" value: \"SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4.2\" } environment { key: \"eigen\" value: \"3.4.90\" } l1_cache_size: 32768 l2_cache_size: 262144 l3_cache_size: 8388608 memory_size: 268435456 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n",
      "2022-07-05 17:36:19.006956: W tensorflow/core/grappler/costs/op_level_cost_estimator.cc:690] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"CPU\" vendor: \"GenuineIntel\" model: \"110\" frequency: 3100 num_cores: 8 environment { key: \"cpu_instruction_set\" value: \"SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4.2\" } environment { key: \"eigen\" value: \"3.4.90\" } l1_cache_size: 32768 l2_cache_size: 262144 l3_cache_size: 8388608 memory_size: 268435456 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n",
      "2022-07-05 17:36:19.339618: W tensorflow/core/grappler/costs/op_level_cost_estimator.cc:690] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"CPU\" vendor: \"GenuineIntel\" model: \"110\" frequency: 3100 num_cores: 8 environment { key: \"cpu_instruction_set\" value: \"SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4.2\" } environment { key: \"eigen\" value: \"3.4.90\" } l1_cache_size: 32768 l2_cache_size: 262144 l3_cache_size: 8388608 memory_size: 268435456 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n",
      "2022-07-05 17:42:46.816862: W tensorflow/core/grappler/costs/op_level_cost_estimator.cc:690] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"CPU\" vendor: \"GenuineIntel\" model: \"110\" frequency: 3100 num_cores: 8 environment { key: \"cpu_instruction_set\" value: \"SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4.2\" } environment { key: \"eigen\" value: \"3.4.90\" } l1_cache_size: 32768 l2_cache_size: 262144 l3_cache_size: 8388608 memory_size: 268435456 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n",
      "2022-07-05 17:42:47.277915: W tensorflow/core/grappler/costs/op_level_cost_estimator.cc:690] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"CPU\" vendor: \"GenuineIntel\" model: \"110\" frequency: 3100 num_cores: 8 environment { key: \"cpu_instruction_set\" value: \"SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4.2\" } environment { key: \"eigen\" value: \"3.4.90\" } l1_cache_size: 32768 l2_cache_size: 262144 l3_cache_size: 8388608 memory_size: 268435456 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n"
     ]
    }
   ],
   "source": [
    "# Extract ELMo embeddings \n",
    "elmo_train = [elmo_vectors(train['clean_text']) for x in list_train] \n",
    "elmo_validation = [elmo_vectors(validation['clean_text']) for x in list_validation]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c23852c",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-07-05T23:04:24.834Z"
    }
   },
   "outputs": [],
   "source": [
    "# Once we have all the vectors, we can concatenate them back to a single array:\n",
    "\n",
    "elmo_X_train = np.concatenate(elmo_train, axis = 0) \n",
    "elmo_X_validation = np.concatenate(elmo_validation, axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d96e8c",
   "metadata": {},
   "source": [
    "# We still need to append the target column from the both of the train and validation dataframes \n",
    "\n",
    "\n",
    "# Both the elmo_X_train and elmo_X_validation need their target columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7afccd67",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d2dbc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving these arrays becuase of the long time it took to get the ELMo vectors \n",
    "# Saving them as pickle files\n",
    "\n",
    "\n",
    "# save elmo_train_new\n",
    "pickle_out = open(\"elmo_X_train.pickle\",\"wb\")\n",
    "pickle.dump(elmo_X_train, pickle_out)\n",
    "pickle_out.close()\n",
    "\n",
    "# save elmo_test_new\n",
    "pickle_out = open(\"elmo_X_validation.pickle\",\"wb\")\n",
    "pickle.dump(elmo_X_validation, pickle_out)\n",
    "pickle_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90966ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # We can use the following code to load them back\n",
    "\n",
    "# # load elmo_train_new\n",
    "# pickle_in = open(\"elmo_X_train.pickle\", \"rb\")\n",
    "# elmo_train_new = pickle.load(pickle_in)\n",
    "\n",
    "# # load elmo_train_new\n",
    "# pickle_in = open(\"elmo_X_validation.pickle\", \"rb\")\n",
    "# elmo_test_new = pickle.load(pickle_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef863ea1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa967cd9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7875c9b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
