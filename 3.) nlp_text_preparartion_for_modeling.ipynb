{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd091f96",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-11T22:22:53.686879Z",
     "start_time": "2022-07-11T22:22:53.683729Z"
    }
   },
   "outputs": [],
   "source": [
    "# Bag Of Words:\n",
    "\n",
    "# We define a fixed length vector where each entry corresponds to a word in our pre-defined dictionary of words. The size of the vector equals the size of the dictionary. \n",
    "# Then, for representing a text using this vector, we count how many times each word of our dictionary appears in the text and we put this number in the corresponding vector entry.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Tf-IDF:\n",
    "\n",
    "# Term frequency-inverse document frequency (TF-IDF) gives a measure that takes the importance of a word into consideration depending on how frequently it occurs in a document and a corpus.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Word to Vector:\n",
    "\n",
    "# Converting words to vectors, or word vectorization, is a natural language processing (NLP) process. \n",
    "# The process uses language models to map words into vector space. A vector space represents each word by a vector of real numbers. It also allows words with similar meanings have similar representations\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Count Vectorizer: \n",
    "\n",
    "# It will fit and learn the word vocabulary and try to create a document term matrix in which the individual cells denote the frequency of that word in a particular document, which is also known as term frequency, and the columns are dedicated to each word in the corpus.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Transfer Learning:\n",
    "\n",
    "# It uses a prebuilt model on your data and gives pretty good results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5483f46",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-11T22:22:54.006193Z",
     "start_time": "2022-07-11T22:22:54.003700Z"
    }
   },
   "outputs": [],
   "source": [
    "# https://medium.com/analytics-vidhya/nlp-tutorial-for-text-classification-in-python-8f19cd17b49e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9960b56d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-11T22:23:07.166339Z",
     "start_time": "2022-07-11T22:22:54.246636Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/sammy/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/sammy/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to /Users/sammy/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# For data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "# For Plotting\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "plt.style.use('ggplot')\n",
    "from matplotlib.pyplot import figure\n",
    "matplotlib.rcParams['figure.figsize'] = (22,10)\n",
    "plt.rcParams.update({'font.size': 18})\n",
    "\n",
    "import seaborn as sns \n",
    "sns.set_style('darkgrid')\n",
    "\n",
    "# For processing\n",
    "import re, string\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from sklearn import feature_extraction, model_selection, naive_bayes, pipeline, manifold, preprocessing, feature_selection\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.metrics import classification_report, f1_score, accuracy_score, confusion_matrix\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
    "\n",
    "\n",
    "# For explainer\n",
    "from lime import lime_text\n",
    "\n",
    "# For word embedding\n",
    "import gensim\n",
    "import gensim.downloader as gensim_api\n",
    "\n",
    "# For deep learning\n",
    "from tensorflow.keras import models, layers, preprocessing as kprocessing\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "# For bert language model\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e3a2f84b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-11T22:23:48.122427Z",
     "start_time": "2022-07-11T22:23:48.041040Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7503 entries, 0 to 7502\n",
      "Data columns (total 16 columns):\n",
      " #   Column                     Non-Null Count  Dtype  \n",
      "---  ------                     --------------  -----  \n",
      " 0   keyword                    7447 non-null   object \n",
      " 1   location                   5021 non-null   object \n",
      " 2   text                       7503 non-null   object \n",
      " 3   target                     7503 non-null   int64  \n",
      " 4   total_words                7503 non-null   int64  \n",
      " 5   char_count                 7503 non-null   int64  \n",
      " 6   sentence_count             7503 non-null   int64  \n",
      " 7   avg_word_length            7503 non-null   float64\n",
      " 8   avg_sentence_lenght        7503 non-null   float64\n",
      " 9   tokenized_text             7503 non-null   object \n",
      " 10  clean_text                 7460 non-null   object \n",
      " 11  clean_total_words          7503 non-null   int64  \n",
      " 12  clean_char_count           7503 non-null   int64  \n",
      " 13  clean_sentence_count       7503 non-null   int64  \n",
      " 14  clean_avg_word_length      7503 non-null   float64\n",
      " 15  clean_avg_sentence_lenght  7503 non-null   float64\n",
      "dtypes: float64(4), int64(7), object(5)\n",
      "memory usage: 938.0+ KB\n"
     ]
    }
   ],
   "source": [
    "# importing dataset\n",
    "\n",
    "data = pd.read_csv('preprocessed_clean_train_set.csv') \n",
    "\n",
    "\n",
    "# Reviewing the data shape, columns and data types\n",
    "\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "877cbb3a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-11T22:23:48.426482Z",
     "start_time": "2022-07-11T22:23:48.380788Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Total</th>\n",
       "      <th>Missing Percent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>location</th>\n",
       "      <td>2482</td>\n",
       "      <td>33.080101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>keyword</th>\n",
       "      <td>56</td>\n",
       "      <td>0.746368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>clean_text</th>\n",
       "      <td>43</td>\n",
       "      <td>0.573104</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Total  Missing Percent\n",
       "location     2482        33.080101\n",
       "keyword        56         0.746368\n",
       "clean_text     43         0.573104"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Percentage of Data Missing Per Column Above 1%\n",
    "\n",
    "total = data.isnull().sum().sort_values(ascending=False)\n",
    "percent = (data.isnull().sum()/data.isnull().count()).sort_values(ascending=False)\n",
    "missing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Missing Percent'])\n",
    "missing_data['Missing Percent'] = missing_data['Missing Percent'].apply(lambda x: x * 100)\n",
    "missing_data.loc[missing_data['Missing Percent'] > .01][:152]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "719047a6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-11T22:23:49.065947Z",
     "start_time": "2022-07-11T22:23:49.036028Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>total_words</th>\n",
       "      <th>char_count</th>\n",
       "      <th>sentence_count</th>\n",
       "      <th>avg_word_length</th>\n",
       "      <th>avg_sentence_lenght</th>\n",
       "      <th>tokenized_text</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>clean_total_words</th>\n",
       "      <th>clean_char_count</th>\n",
       "      <th>clean_sentence_count</th>\n",
       "      <th>clean_avg_word_length</th>\n",
       "      <th>clean_avg_sentence_lenght</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>this is ridiculous....</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>20</td>\n",
       "      <td>5</td>\n",
       "      <td>6.666667</td>\n",
       "      <td>0.6</td>\n",
       "      <td>['this', 'is', 'ridiculous', '...']</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LOOOOOOL</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>['LOOOOOOL']</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>accident</td>\n",
       "      <td>NaN</td>\n",
       "      <td>???? it was an accident http://t.co/Oia5fxi4gM</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>41</td>\n",
       "      <td>2</td>\n",
       "      <td>6.833333</td>\n",
       "      <td>3.0</td>\n",
       "      <td>['?', '?', '?', 'it', 'was', 'an', 'accident',...</td>\n",
       "      <td>accident</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>accident</td>\n",
       "      <td>Alberta | Sask. | Montana</td>\n",
       "      <td>Suffield Alberta Accident https://t.co/bPTmlF4P10</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>46</td>\n",
       "      <td>2</td>\n",
       "      <td>11.500000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>['Suffield', 'Alberta', 'Accident', 'https://t...</td>\n",
       "      <td>accident</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>accident</td>\n",
       "      <td>Gloucestershire , UK</td>\n",
       "      <td>@flowri were you marinading it or was it an ac...</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>44</td>\n",
       "      <td>1</td>\n",
       "      <td>4.400000</td>\n",
       "      <td>10.0</td>\n",
       "      <td>['@flowri', 'were', 'you', 'marinading', 'it',...</td>\n",
       "      <td>accident</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7492</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>#??? #?? #??? #??? MH370: Aircraft debris foun...</td>\n",
       "      <td>1</td>\n",
       "      <td>18</td>\n",
       "      <td>105</td>\n",
       "      <td>5</td>\n",
       "      <td>5.833333</td>\n",
       "      <td>3.6</td>\n",
       "      <td>['#', '?', '?', '?', '#', '?', '?', '#', '?', ...</td>\n",
       "      <td>aircraft debris find la reunion miss malaysia ...</td>\n",
       "      <td>8</td>\n",
       "      <td>47</td>\n",
       "      <td>1</td>\n",
       "      <td>5.875000</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7495</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>#breaking #LA Refugio oil spill may have been ...</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>87</td>\n",
       "      <td>2</td>\n",
       "      <td>6.692308</td>\n",
       "      <td>6.5</td>\n",
       "      <td>['#breaking', '#LA', 'Refugio', 'oil', 'spill'...</td>\n",
       "      <td>refugio oil spill costlier big project</td>\n",
       "      <td>6</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "      <td>5.500000</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7498</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>#WorldNews Fallen powerlines on G:link tram: U...</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>118</td>\n",
       "      <td>5</td>\n",
       "      <td>6.210526</td>\n",
       "      <td>3.8</td>\n",
       "      <td>['#WorldNews', 'Fallen', 'powerlines', 'on', '...</td>\n",
       "      <td>fall pipeline link tram update fire crew evacu...</td>\n",
       "      <td>9</td>\n",
       "      <td>51</td>\n",
       "      <td>1</td>\n",
       "      <td>5.666667</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7501</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Two giant cranes holding a bridge collapse int...</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>73</td>\n",
       "      <td>2</td>\n",
       "      <td>6.636364</td>\n",
       "      <td>5.5</td>\n",
       "      <td>['Two', 'giant', 'cranes', 'holding', 'a', 'br...</td>\n",
       "      <td>giant crane hold bridge collapse nearby home</td>\n",
       "      <td>7</td>\n",
       "      <td>38</td>\n",
       "      <td>1</td>\n",
       "      <td>5.428571</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7502</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The Latest: More Homes Razed by Northern Calif...</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>82</td>\n",
       "      <td>2</td>\n",
       "      <td>6.307692</td>\n",
       "      <td>6.5</td>\n",
       "      <td>['The', 'Latest', ':', 'More', 'Homes', 'Razed...</td>\n",
       "      <td>late home raze northern california wildfire ac...</td>\n",
       "      <td>8</td>\n",
       "      <td>45</td>\n",
       "      <td>1</td>\n",
       "      <td>5.625000</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1075 rows × 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       keyword                   location  \\\n",
       "20         NaN                        NaN   \n",
       "24         NaN                        NaN   \n",
       "89    accident                        NaN   \n",
       "93    accident  Alberta | Sask. | Montana   \n",
       "97    accident       Gloucestershire , UK   \n",
       "...        ...                        ...   \n",
       "7492       NaN                        NaN   \n",
       "7495       NaN                        NaN   \n",
       "7498       NaN                        NaN   \n",
       "7501       NaN                        NaN   \n",
       "7502       NaN                        NaN   \n",
       "\n",
       "                                                   text  target  total_words  \\\n",
       "20                               this is ridiculous....       0            3   \n",
       "24                                             LOOOOOOL       0            1   \n",
       "89       ???? it was an accident http://t.co/Oia5fxi4gM       0            6   \n",
       "93    Suffield Alberta Accident https://t.co/bPTmlF4P10       1            4   \n",
       "97    @flowri were you marinading it or was it an ac...       0           10   \n",
       "...                                                 ...     ...          ...   \n",
       "7492  #??? #?? #??? #??? MH370: Aircraft debris foun...       1           18   \n",
       "7495  #breaking #LA Refugio oil spill may have been ...       1           13   \n",
       "7498  #WorldNews Fallen powerlines on G:link tram: U...       1           19   \n",
       "7501  Two giant cranes holding a bridge collapse int...       1           11   \n",
       "7502  The Latest: More Homes Razed by Northern Calif...       1           13   \n",
       "\n",
       "      char_count  sentence_count  avg_word_length  avg_sentence_lenght  \\\n",
       "20            20               5         6.666667                  0.6   \n",
       "24             8               1         8.000000                  1.0   \n",
       "89            41               2         6.833333                  3.0   \n",
       "93            46               2        11.500000                  2.0   \n",
       "97            44               1         4.400000                 10.0   \n",
       "...          ...             ...              ...                  ...   \n",
       "7492         105               5         5.833333                  3.6   \n",
       "7495          87               2         6.692308                  6.5   \n",
       "7498         118               5         6.210526                  3.8   \n",
       "7501          73               2         6.636364                  5.5   \n",
       "7502          82               2         6.307692                  6.5   \n",
       "\n",
       "                                         tokenized_text  \\\n",
       "20                  ['this', 'is', 'ridiculous', '...']   \n",
       "24                                         ['LOOOOOOL']   \n",
       "89    ['?', '?', '?', 'it', 'was', 'an', 'accident',...   \n",
       "93    ['Suffield', 'Alberta', 'Accident', 'https://t...   \n",
       "97    ['@flowri', 'were', 'you', 'marinading', 'it',...   \n",
       "...                                                 ...   \n",
       "7492  ['#', '?', '?', '?', '#', '?', '?', '#', '?', ...   \n",
       "7495  ['#breaking', '#LA', 'Refugio', 'oil', 'spill'...   \n",
       "7498  ['#WorldNews', 'Fallen', 'powerlines', 'on', '...   \n",
       "7501  ['Two', 'giant', 'cranes', 'holding', 'a', 'br...   \n",
       "7502  ['The', 'Latest', ':', 'More', 'Homes', 'Razed...   \n",
       "\n",
       "                                             clean_text  clean_total_words  \\\n",
       "20                                                  NaN                  1   \n",
       "24                                                  NaN                  1   \n",
       "89                                             accident                  1   \n",
       "93                                             accident                  1   \n",
       "97                                             accident                  1   \n",
       "...                                                 ...                ...   \n",
       "7492  aircraft debris find la reunion miss malaysia ...                  8   \n",
       "7495             refugio oil spill costlier big project                  6   \n",
       "7498  fall pipeline link tram update fire crew evacu...                  9   \n",
       "7501       giant crane hold bridge collapse nearby home                  7   \n",
       "7502  late home raze northern california wildfire ac...                  8   \n",
       "\n",
       "      clean_char_count  clean_sentence_count  clean_avg_word_length  \\\n",
       "20                   0                     1               0.000000   \n",
       "24                   0                     1               0.000000   \n",
       "89                   8                     1               8.000000   \n",
       "93                   8                     1               8.000000   \n",
       "97                   8                     1               8.000000   \n",
       "...                ...                   ...                    ...   \n",
       "7492                47                     1               5.875000   \n",
       "7495                33                     1               5.500000   \n",
       "7498                51                     1               5.666667   \n",
       "7501                38                     1               5.428571   \n",
       "7502                45                     1               5.625000   \n",
       "\n",
       "      clean_avg_sentence_lenght  \n",
       "20                          1.0  \n",
       "24                          1.0  \n",
       "89                          1.0  \n",
       "93                          1.0  \n",
       "97                          1.0  \n",
       "...                         ...  \n",
       "7492                        8.0  \n",
       "7495                        6.0  \n",
       "7498                        9.0  \n",
       "7501                        7.0  \n",
       "7502                        8.0  \n",
       "\n",
       "[1075 rows x 16 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking for Duplicates \n",
    "\n",
    "data[data.duplicated(subset = \"clean_text\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d95b132f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-11T22:23:52.343083Z",
     "start_time": "2022-07-11T22:23:52.336946Z"
    }
   },
   "outputs": [],
   "source": [
    "# Dropping Rows with any missing values\n",
    "\n",
    "data.dropna(axis = 0, subset = ['clean_text'], inplace = True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "34c8efee",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-11T22:23:52.738427Z",
     "start_time": "2022-07-11T22:23:52.715560Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Total</th>\n",
       "      <th>Missing Percent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>location</th>\n",
       "      <td>2458</td>\n",
       "      <td>32.949062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>keyword</th>\n",
       "      <td>53</td>\n",
       "      <td>0.710456</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Total  Missing Percent\n",
       "location   2458        32.949062\n",
       "keyword      53         0.710456"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Percentage of Data Missing Per Column Above 1%\n",
    "\n",
    "total = data.isnull().sum().sort_values(ascending=False)\n",
    "percent = (data.isnull().sum()/data.isnull().count()).sort_values(ascending=False)\n",
    "missing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Missing Percent'])\n",
    "missing_data['Missing Percent'] = missing_data['Missing Percent'].apply(lambda x: x * 100)\n",
    "missing_data.loc[missing_data['Missing Percent'] > .01][:152]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6954b8f4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-11T22:23:07.763977Z",
     "start_time": "2022-07-11T22:23:07.763948Z"
    }
   },
   "outputs": [],
   "source": [
    "necessary_columns = data[['clean_text', 'target']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b973e3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-11T22:23:07.768342Z",
     "start_time": "2022-07-11T22:23:07.768321Z"
    }
   },
   "outputs": [],
   "source": [
    "# Creating a validation dataframe with 25%\n",
    "# values of original dataframe\n",
    "validation = necessary_columns.sample(frac = 0.25)\n",
    " \n",
    "# Creating dataframe with the 75 % remaing values\n",
    "train = necessary_columns.drop(validation.index)\n",
    " \n",
    "print(\"Validation of the given DataFrame:\")\n",
    "print(validation.info())\n",
    " \n",
    "print(\"Train set of the given DataFrame:\")\n",
    "print(train.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "04ffe2cb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-11T22:24:30.023821Z",
     "start_time": "2022-07-11T22:24:30.012466Z"
    }
   },
   "outputs": [],
   "source": [
    "# splitting the data into a train and validation\n",
    "\n",
    "X_train, X_validation, y_train, y_validation = train_test_split(data['clean_text'], \n",
    "                                                                data['target'], \n",
    "                                                                test_size = 0.25, \n",
    "                                                                shuffle = True,\n",
    "                                                                random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2626fbbf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-11T23:17:57.336800Z",
     "start_time": "2022-07-11T22:50:43.401314Z"
    }
   },
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "bert = SentenceTransformer('stsb-roberta-large') #1.3 gb\n",
    "\n",
    "\n",
    "# Vectorize the data\n",
    "\n",
    "X_train_vec = pd.DataFrame(np.vstack(X_train.apply(bert.encode)))\n",
    "\n",
    "\n",
    "X_validation_vec = pd.DataFrame(np.vstack(X_validation.apply(bert.encode)))\n",
    "\n",
    "# BERT doesn't have feature names\n",
    "\n",
    "# model = RandomForestClassifier(n_estimators=500, n_jobs=8)\n",
    "# model.fit(X_train_vec, y_train)\n",
    "# model.score(X_validation_vec, y_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8baf3747",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-11T23:45:11.646833Z",
     "start_time": "2022-07-11T23:45:11.636023Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5595, 1024)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(1865, 1024)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(5595,)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(1865,)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display(X_train_vec.shape)\n",
    "display(X_validation_vec.shape)\n",
    "display(y_train.shape)\n",
    "y_validation.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6f17223e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-12T00:04:21.550998Z",
     "start_time": "2022-07-12T00:04:21.546495Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X_train_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "66ae06a9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-12T00:07:29.660137Z",
     "start_time": "2022-07-12T00:07:23.695881Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train_vec.to_csv('X_train_vec.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6eeca94",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-12T00:07:29.831764Z",
     "start_time": "2022-07-12T00:07:29.831734Z"
    }
   },
   "outputs": [],
   "source": [
    "X_validation_vec.to_csv('X_validation_vec.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fdfc4e4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-12T00:07:29.836717Z",
     "start_time": "2022-07-12T00:07:29.836673Z"
    }
   },
   "outputs": [],
   "source": [
    "y_train.to_csv('y_train.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b0f9470",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-12T00:07:29.841813Z",
     "start_time": "2022-07-12T00:07:29.841757Z"
    }
   },
   "outputs": [],
   "source": [
    "y_validation.to_csv('y_validation.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
