{
 "cells": [
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 6,
=======
   "execution_count": 85,
   "id": "c02565be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plan of Attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "2bedd56d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SR\n",
    "## Finish EDA and Preprocessing Pickle File and Steps\n",
    "## Gensim // Spacy pretrained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ddbe7ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NG\n",
    "## Gridsearching - Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d512ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
>>>>>>> ng_branch
   "id": "006625c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove words of size 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b659a51b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Functions import *"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 2,
=======
   "execution_count": 4,
>>>>>>> ng_branch
   "id": "44edaf63",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "import nltk\n",
    "from nltk import word_tokenize, FreqDist\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 7,
=======
   "execution_count": 5,
>>>>>>> ng_branch
   "id": "3b1f10ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r'preprocessed_clean_text.csv')"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 10,
   "id": "05eeb474",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['id'] not found in axis\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\NEL-JI~1\\AppData\\Local\\Temp/ipykernel_3524/3508027962.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'id'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'keyword'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'location'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m                 )\n\u001b[1;32m--> 311\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    312\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36mdrop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   4904\u001b[0m                 \u001b[0mweight\u001b[0m  \u001b[1;36m1.0\u001b[0m     \u001b[1;36m0.8\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4905\u001b[0m         \"\"\"\n\u001b[1;32m-> 4906\u001b[1;33m         return super().drop(\n\u001b[0m\u001b[0;32m   4907\u001b[0m             \u001b[0mlabels\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4908\u001b[0m             \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36mdrop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   4148\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;32min\u001b[0m \u001b[0maxes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4149\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4150\u001b[1;33m                 \u001b[0mobj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_drop_axis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4151\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4152\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0minplace\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m_drop_axis\u001b[1;34m(self, labels, axis, level, errors)\u001b[0m\n\u001b[0;32m   4183\u001b[0m                 \u001b[0mnew_axis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4184\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4185\u001b[1;33m                 \u001b[0mnew_axis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4186\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreindex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0maxis_name\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mnew_axis\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4187\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mdrop\u001b[1;34m(self, labels, errors)\u001b[0m\n\u001b[0;32m   6015\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mmask\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0many\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6016\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0merrors\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m\"ignore\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 6017\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"{labels[mask]} not found in axis\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   6018\u001b[0m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m~\u001b[0m\u001b[0mmask\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6019\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdelete\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: \"['id'] not found in axis\""
     ]
    }
   ],
   "source": [
    "df.drop(['id','keyword','location'],axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "97475d28",
   "metadata": {
    "scrolled": true
   },
=======
   "execution_count": 6,
   "id": "f625fb46",
   "metadata": {},
>>>>>>> ng_branch
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
<<<<<<< HEAD
=======
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
>>>>>>> ng_branch
       "      <th>target</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
<<<<<<< HEAD
=======
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
>>>>>>> ng_branch
       "      <td>1</td>\n",
       "      <td>deed reason allah forgive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
<<<<<<< HEAD
=======
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
>>>>>>> ng_branch
       "      <td>1</td>\n",
       "      <td>forest fire near la range sask canada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
<<<<<<< HEAD
=======
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
>>>>>>> ng_branch
       "      <td>1</td>\n",
       "      <td>resident ask shelter place notify officer evac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
<<<<<<< HEAD
=======
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
>>>>>>> ng_branch
       "      <td>1</td>\n",
       "      <td>people receive evacuation order california</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
<<<<<<< HEAD
=======
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
>>>>>>> ng_branch
       "      <td>1</td>\n",
       "      <td>get send photo ruby smoke pour school</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
<<<<<<< HEAD
       "   target                                               text\n",
       "0       1                          deed reason allah forgive\n",
       "1       1              forest fire near la range sask canada\n",
       "2       1  resident ask shelter place notify officer evac...\n",
       "3       1         people receive evacuation order california\n",
       "4       1              get send photo ruby smoke pour school"
      ]
     },
     "execution_count": 11,
=======
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  "
      ]
     },
     "execution_count": 6,
>>>>>>> ng_branch
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
   "id": "15219185",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessor(dataframe):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7ef1cd2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_pos(treebank_tag):\n",
    "\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return ''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a9391f4",
   "metadata": {},
   "source": [
    "# Tweet Tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d678ab12",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweeter = TweetTokenizer(strip_handles=True,reduce_len=True, preserve_case=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fc2f41f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tweets'] = df['text'].apply(lambda text: tweeter.tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "50ee6607",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       [our, deeds, are, the, reason, of, this, #eart...\n",
       "1        [forest, fire, near, la, ronge, sask, ., canada]\n",
       "2       [all, residents, asked, to, ', shelter, in, pl...\n",
       "3       [13,000, people, receive, #wildfires, evacuati...\n",
       "4       [just, got, sent, this, photo, from, ruby, #al...\n",
       "                              ...                        \n",
       "7608    [two, giant, cranes, holding, a, bridge, colla...\n",
       "7609    [the, out, of, control, wild, fires, in, calif...\n",
       "7610    [m1, ., 94, [, 01:04, utc, ], ?, 5km, s, of, v...\n",
       "7611    [police, investigating, after, an, e-bike, col...\n",
       "7612    [the, latest, :, more, homes, razed, by, north...\n",
       "Name: tweets, Length: 7613, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['tweets']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "40bb83e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemma = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fb79eb11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatizing(sentence_array):\n",
    "    lemmas = nltk.pos_tag(sentence_array)\n",
    "    container = []\n",
    "    for word,lem in lemmas:\n",
    "        pos = get_wordnet_pos(lem)\n",
    "        if pos:\n",
    "            container.append(lemma.lemmatize(word,pos=pos))\n",
    "        else:\n",
    "            container.append(lemma.lemmatize(word))\n",
    "    return container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ec6c5038",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['lemmed'] = df['tweets'].apply(lambda array: lemmatizing(array))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "639aab04",
=======
   "execution_count": 11,
   "id": "9e1b59a1",
>>>>>>> ng_branch
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "110"
      ]
     },
<<<<<<< HEAD
     "execution_count": 16,
=======
     "execution_count": 11,
>>>>>>> ng_branch
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
<<<<<<< HEAD
    "df['lemmed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b896cc2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaner(sentence_array):\n",
    "    sentence = \" \".join(sentence_array)\n",
    "    sentence = re.sub(r\"http\\S+\", \"\", sentence)\n",
    "    sentence = re.sub('[()!?]', ' ', sentence)\n",
    "    sentence = re.sub('[()!?]', ' ', sentence)\n",
    "    sentence = re.sub('\\[.*?\\]',' ', sentence)\n",
    "    sentence = re.sub(\"[^a-z0-9]\",\" \", sentence)\n",
    "    sentence = re.sub(r'\\b\\w{1,3}\\b', '', sentence)\n",
    "    sentence = [w for w in sentence.split() if not w in stopwords.words('english')]\n",
    "\n",
    "    return \" \".join(sentence)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "17f7ae0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['cleaned'] = df['lemmed'].apply(lambda array: cleaner(array))"
=======
    "df['text'].duplicated().sum()"
>>>>>>> ng_branch
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 19,
   "id": "601a752f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                    deed reason earthquake allah forgive\n",
       "1                      forest fire near ronge sask canada\n",
       "2       resident shelter place notify officer evacuati...\n",
       "3       people receive wildfires evacuation order cali...\n",
       "4       send photo ruby alaska smoke wildfires pour sc...\n",
       "                              ...                        \n",
       "7608         giant crane hold bridge collapse nearby home\n",
       "7609    control wild fire california even northern par...\n",
       "7610                                       volcano hawaii\n",
       "7611    police investigate bike collided little portug...\n",
       "7612     late home raze northern california wildfire news\n",
       "Name: cleaned, Length: 7613, dtype: object"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['cleaned']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e193f330",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['cleaned'].isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "386b3ea2",
   "metadata": {},
   "source": [
    "# Machine learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "31065006",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#reminder to build preprocessor function and insert in the vectorizers instead of using the default\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "from keras import models, layers, optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "05eb3b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df['cleaned'],df['target'],test_size = .2, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "467a41bf",
   "metadata": {},
   "source": [
    "## CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "225a3b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0c0ab0a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "counted_vect = counter.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "95637951",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counted_vect.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8c15ac7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "counted_df = pd.DataFrame(data=counted_vect.todense(),columns=counter.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f94e31d7",
=======
   "execution_count": 14,
   "id": "18f06e73",
>>>>>>> ng_branch
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
<<<<<<< HEAD
       "      <th>aaaaaaallll</th>\n",
       "      <th>aaarrrgghhh</th>\n",
       "      <th>ab</th>\n",
       "      <th>aba</th>\n",
       "      <th>abandon</th>\n",
       "      <th>abbott</th>\n",
       "      <th>abbswinston</th>\n",
       "      <th>abe</th>\n",
       "      <th>aberdeen</th>\n",
       "      <th>ability</th>\n",
       "      <th>...</th>\n",
       "      <th>zionist</th>\n",
       "      <th>zionists</th>\n",
       "      <th>zip</th>\n",
       "      <th>zipped</th>\n",
       "      <th>zipper</th>\n",
       "      <th>zombie</th>\n",
       "      <th>zone</th>\n",
       "      <th>zoom</th>\n",
       "      <th>zurich</th>\n",
       "      <th>zzz</th>\n",
=======
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
>>>>>>> ng_branch
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>68</td>\n",
       "      <td>ablaze</td>\n",
       "      <td>Live On Webcam</td>\n",
       "      <td>Check these out: http://t.co/rOI2NSmEJJ http:/...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>165</td>\n",
       "      <td>aftershock</td>\n",
       "      <td>US</td>\n",
       "      <td>320 [IR] ICEMOON [AFTERSHOCK] | http://t.co/vA...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>172</td>\n",
       "      <td>aftershock</td>\n",
       "      <td>Switzerland</td>\n",
       "      <td>320 [IR] ICEMOON [AFTERSHOCK] | http://t.co/TH...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>238</td>\n",
       "      <td>airplane%20accident</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Experts in France begin examining airplane deb...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>624</th>\n",
       "      <td>898</td>\n",
       "      <td>bioterrorism</td>\n",
       "      <td>NaN</td>\n",
       "      <td>To fight bioterrorism sir.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>630</th>\n",
       "      <td>907</td>\n",
       "      <td>bioterrorism</td>\n",
       "      <td>NaN</td>\n",
       "      <td>To fight bioterrorism sir.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>634</th>\n",
       "      <td>916</td>\n",
       "      <td>bioterrorism</td>\n",
       "      <td>NaN</td>\n",
       "      <td>To fight bioterrorism sir.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1156</th>\n",
       "      <td>1665</td>\n",
       "      <td>bombing</td>\n",
       "      <td>Singapore</td>\n",
       "      <td>Japan on Thursday marks the 70th anniversary o...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1251</th>\n",
       "      <td>1807</td>\n",
       "      <td>buildings%20on%20fire</td>\n",
       "      <td>UK</td>\n",
       "      <td>#TweetLikeItsSeptember11th2001 Those two build...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1331</th>\n",
       "      <td>1922</td>\n",
       "      <td>burning%20buildings</td>\n",
       "      <td>Mackay, QLD, Australia</td>\n",
       "      <td>Mmmmmm I'm burning.... I'm burning buildings I...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1332</th>\n",
       "      <td>1924</td>\n",
       "      <td>burning%20buildings</td>\n",
       "      <td>St Charles, MD</td>\n",
       "      <td>I'm mentally preparing myself for a bomb ass s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1335</th>\n",
       "      <td>1929</td>\n",
       "      <td>burning%20buildings</td>\n",
       "      <td>NaN</td>\n",
       "      <td>@fewmoretweets all lives matter. Just not a fa...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1343</th>\n",
       "      <td>1941</td>\n",
       "      <td>burning%20buildings</td>\n",
       "      <td>NJ</td>\n",
       "      <td>@themagickidraps not upset with a rally upset ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1345</th>\n",
       "      <td>1943</td>\n",
       "      <td>burning%20buildings</td>\n",
       "      <td>New Orleans ,Louisiana</td>\n",
       "      <td>Burning buildings? Media outrage? http://t.co/...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1349</th>\n",
       "      <td>1950</td>\n",
       "      <td>burning%20buildings</td>\n",
       "      <td>dallas</td>\n",
       "      <td>like for the music video I want some real acti...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1356</th>\n",
       "      <td>1957</td>\n",
       "      <td>burning%20buildings</td>\n",
       "      <td>Madison, GA</td>\n",
       "      <td>@_minimehh @cjoyner I must be overlooking the ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1360</th>\n",
       "      <td>1962</td>\n",
       "      <td>burning%20buildings</td>\n",
       "      <td>In Hell</td>\n",
       "      <td>Schools in Western Uganda still Burning down B...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1365</th>\n",
       "      <td>1968</td>\n",
       "      <td>burning%20buildings</td>\n",
       "      <td>Epic City, BB.</td>\n",
       "      <td>I Pledge Allegiance To The P.O.P.E. And The Bu...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1623</th>\n",
       "      <td>2346</td>\n",
       "      <td>collapse</td>\n",
       "      <td>Mumbai , India</td>\n",
       "      <td>Warne shocked over Australia's epic collapse a...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1725</th>\n",
       "      <td>2488</td>\n",
       "      <td>collided</td>\n",
       "      <td>Peterborough, Ont.</td>\n",
       "      <td>#Newswatch: 2 vehicles collided at Lock and La...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2352</th>\n",
       "      <td>3387</td>\n",
       "      <td>demolition</td>\n",
       "      <td>NaN</td>\n",
       "      <td>General News Û¢åÊ'Demolition of houses on wat...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2441</th>\n",
       "      <td>3505</td>\n",
       "      <td>derailment</td>\n",
       "      <td>India</td>\n",
       "      <td>Madhya Pradesh Train Derailment: Village Youth...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2449</th>\n",
       "      <td>3517</td>\n",
       "      <td>derailment</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Madhya Pradesh Train Derailment: Village Youth...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2452</th>\n",
       "      <td>3520</td>\n",
       "      <td>derailment</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Madhya Pradesh Train Derailment: Village Youth...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2454</th>\n",
       "      <td>3522</td>\n",
       "      <td>derailment</td>\n",
       "      <td>India</td>\n",
       "      <td>Madhya Pradesh Train Derailment: Village Youth...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2456</th>\n",
       "      <td>3524</td>\n",
       "      <td>derailment</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Madhya Pradesh Train Derailment: Village Youth...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2477</th>\n",
       "      <td>3552</td>\n",
       "      <td>derailment</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Madhya Pradesh Train Derailment: Village Youth...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2655</th>\n",
       "      <td>3814</td>\n",
       "      <td>detonate</td>\n",
       "      <td>Morioh, Japan</td>\n",
       "      <td>@spinningbot Are you another Stand-user? If yo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2666</th>\n",
       "      <td>3828</td>\n",
       "      <td>detonate</td>\n",
       "      <td>Morioh, Japan</td>\n",
       "      <td>@TinyJecht Are you another Stand-user? If you ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2674</th>\n",
       "      <td>3836</td>\n",
       "      <td>detonate</td>\n",
       "      <td>Morioh, Japan</td>\n",
       "      <td>@spinningbot Are you another Stand-user? If yo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2679</th>\n",
       "      <td>3841</td>\n",
       "      <td>detonate</td>\n",
       "      <td>Morioh, Japan</td>\n",
       "      <td>@TinyJecht Are you another Stand-user? If you ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2828</th>\n",
       "      <td>4064</td>\n",
       "      <td>displaced</td>\n",
       "      <td>NaN</td>\n",
       "      <td>#KCA #VoteJKT48ID 12News: UPDATE: A family of ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2831</th>\n",
       "      <td>4072</td>\n",
       "      <td>displaced</td>\n",
       "      <td>Pedophile hunting ground</td>\n",
       "      <td>.POTUS #StrategicPatience is a strategy for #G...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2832</th>\n",
       "      <td>4076</td>\n",
       "      <td>displaced</td>\n",
       "      <td>Pedophile hunting ground</td>\n",
       "      <td>.POTUS #StrategicPatience is a strategy for #G...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2833</th>\n",
       "      <td>4077</td>\n",
       "      <td>displaced</td>\n",
       "      <td>Pedophile hunting ground</td>\n",
       "      <td>.POTUS #StrategicPatience is a strategy for #G...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2841</th>\n",
       "      <td>4086</td>\n",
       "      <td>displaced</td>\n",
       "      <td>Pedophile hunting ground</td>\n",
       "      <td>#Myanmar  Displaced #Rohingya at #Sittwe point...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3243</th>\n",
       "      <td>4659</td>\n",
       "      <td>engulfed</td>\n",
       "      <td>Kuwait</td>\n",
       "      <td>He came to a land which was engulfed in tribal...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3248</th>\n",
       "      <td>4669</td>\n",
       "      <td>engulfed</td>\n",
       "      <td>Bahrain</td>\n",
       "      <td>He came to a land which was engulfed in tribal...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3251</th>\n",
       "      <td>4672</td>\n",
       "      <td>engulfed</td>\n",
       "      <td>NaN</td>\n",
       "      <td>He came to a land which was engulfed in tribal...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3261</th>\n",
       "      <td>4684</td>\n",
       "      <td>engulfed</td>\n",
       "      <td>NaN</td>\n",
       "      <td>He came to a land which was engulfed in tribal...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3266</th>\n",
       "      <td>4691</td>\n",
       "      <td>engulfed</td>\n",
       "      <td>NaN</td>\n",
       "      <td>He came to a land which was engulfed in tribal...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3374</th>\n",
       "      <td>4832</td>\n",
       "      <td>evacuation</td>\n",
       "      <td>Na:tinixw / Hoopa, Berkeley</td>\n",
       "      <td>Elem Pomo helping the displaced from the Rocky...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3382</th>\n",
       "      <td>4844</td>\n",
       "      <td>evacuation</td>\n",
       "      <td>Renfrew, Scotland</td>\n",
       "      <td>@batfanuk we enjoyed the show today. Great fun...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3390</th>\n",
       "      <td>4853</td>\n",
       "      <td>evacuation</td>\n",
       "      <td>Portland, Ore.</td>\n",
       "      <td>New evacuation ordered for 25 homes in danger ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3461</th>\n",
       "      <td>4952</td>\n",
       "      <td>exploded</td>\n",
       "      <td>NaN</td>\n",
       "      <td>that exploded &amp;amp; brought about the\\r\\nbegin...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3589</th>\n",
       "      <td>5127</td>\n",
       "      <td>fatal</td>\n",
       "      <td>Varanasi</td>\n",
       "      <td>11-Year-Old Boy Charged With Manslaughter of T...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3591</th>\n",
       "      <td>5130</td>\n",
       "      <td>fatal</td>\n",
       "      <td>Thane</td>\n",
       "      <td>11-Year-Old Boy Charged With Manslaughter of T...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3597</th>\n",
       "      <td>5137</td>\n",
       "      <td>fatal</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11-Year-Old Boy Charged With Manslaughter of T...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3600</th>\n",
       "      <td>5140</td>\n",
       "      <td>fatal</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11-Year-Old Boy Charged With Manslaughter of T...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3603</th>\n",
       "      <td>5144</td>\n",
       "      <td>fatal</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11-Year-Old Boy Charged With Manslaughter of T...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                keyword                     location  \\\n",
       "48      68                 ablaze               Live On Webcam   \n",
       "115    165             aftershock                           US   \n",
       "119    172             aftershock                  Switzerland   \n",
       "164    238    airplane%20accident                          NaN   \n",
       "624    898           bioterrorism                          NaN   \n",
       "630    907           bioterrorism                          NaN   \n",
       "634    916           bioterrorism                          NaN   \n",
       "1156  1665                bombing                    Singapore   \n",
       "1251  1807  buildings%20on%20fire                           UK   \n",
       "1331  1922    burning%20buildings       Mackay, QLD, Australia   \n",
       "1332  1924    burning%20buildings               St Charles, MD   \n",
       "1335  1929    burning%20buildings                          NaN   \n",
       "1343  1941    burning%20buildings                           NJ   \n",
       "1345  1943    burning%20buildings       New Orleans ,Louisiana   \n",
       "1349  1950    burning%20buildings                       dallas   \n",
       "1356  1957    burning%20buildings                  Madison, GA   \n",
       "1360  1962    burning%20buildings                      In Hell   \n",
       "1365  1968    burning%20buildings               Epic City, BB.   \n",
       "1623  2346               collapse               Mumbai , India   \n",
       "1725  2488               collided           Peterborough, Ont.   \n",
       "2352  3387             demolition                          NaN   \n",
       "2441  3505             derailment                        India   \n",
       "2449  3517             derailment                          NaN   \n",
       "2452  3520             derailment                          NaN   \n",
       "2454  3522             derailment                        India   \n",
       "2456  3524             derailment                          NaN   \n",
       "2477  3552             derailment                          NaN   \n",
       "2655  3814               detonate                Morioh, Japan   \n",
       "2666  3828               detonate                Morioh, Japan   \n",
       "2674  3836               detonate                Morioh, Japan   \n",
       "2679  3841               detonate                Morioh, Japan   \n",
       "2828  4064              displaced                          NaN   \n",
       "2831  4072              displaced     Pedophile hunting ground   \n",
       "2832  4076              displaced     Pedophile hunting ground   \n",
       "2833  4077              displaced     Pedophile hunting ground   \n",
       "2841  4086              displaced     Pedophile hunting ground   \n",
       "3243  4659               engulfed                      Kuwait    \n",
       "3248  4669               engulfed                      Bahrain   \n",
       "3251  4672               engulfed                          NaN   \n",
       "3261  4684               engulfed                          NaN   \n",
       "3266  4691               engulfed                          NaN   \n",
       "3374  4832             evacuation  Na:tinixw / Hoopa, Berkeley   \n",
       "3382  4844             evacuation            Renfrew, Scotland   \n",
       "3390  4853             evacuation              Portland, Ore.    \n",
       "3461  4952               exploded                          NaN   \n",
       "3589  5127                  fatal                     Varanasi   \n",
       "3591  5130                  fatal                        Thane   \n",
       "3597  5137                  fatal                          NaN   \n",
       "3600  5140                  fatal                          NaN   \n",
       "3603  5144                  fatal                          NaN   \n",
       "\n",
       "                                                   text  target  \n",
       "48    Check these out: http://t.co/rOI2NSmEJJ http:/...       0  \n",
       "115   320 [IR] ICEMOON [AFTERSHOCK] | http://t.co/vA...       0  \n",
       "119   320 [IR] ICEMOON [AFTERSHOCK] | http://t.co/TH...       0  \n",
       "164   Experts in France begin examining airplane deb...       1  \n",
       "624                          To fight bioterrorism sir.       0  \n",
       "630                          To fight bioterrorism sir.       1  \n",
       "634                          To fight bioterrorism sir.       0  \n",
       "1156  Japan on Thursday marks the 70th anniversary o...       1  \n",
       "1251  #TweetLikeItsSeptember11th2001 Those two build...       1  \n",
       "1331  Mmmmmm I'm burning.... I'm burning buildings I...       0  \n",
       "1332  I'm mentally preparing myself for a bomb ass s...       0  \n",
       "1335  @fewmoretweets all lives matter. Just not a fa...       1  \n",
       "1343  @themagickidraps not upset with a rally upset ...       1  \n",
       "1345  Burning buildings? Media outrage? http://t.co/...       1  \n",
       "1349  like for the music video I want some real acti...       0  \n",
       "1356  @_minimehh @cjoyner I must be overlooking the ...       1  \n",
       "1360  Schools in Western Uganda still Burning down B...       1  \n",
       "1365  I Pledge Allegiance To The P.O.P.E. And The Bu...       1  \n",
       "1623  Warne shocked over Australia's epic collapse a...       1  \n",
       "1725  #Newswatch: 2 vehicles collided at Lock and La...       1  \n",
       "2352  General News Û¢åÊ'Demolition of houses on wat...       0  \n",
       "2441  Madhya Pradesh Train Derailment: Village Youth...       1  \n",
       "2449  Madhya Pradesh Train Derailment: Village Youth...       1  \n",
       "2452  Madhya Pradesh Train Derailment: Village Youth...       1  \n",
       "2454  Madhya Pradesh Train Derailment: Village Youth...       1  \n",
       "2456  Madhya Pradesh Train Derailment: Village Youth...       1  \n",
       "2477  Madhya Pradesh Train Derailment: Village Youth...       1  \n",
       "2655  @spinningbot Are you another Stand-user? If yo...       0  \n",
       "2666  @TinyJecht Are you another Stand-user? If you ...       0  \n",
       "2674  @spinningbot Are you another Stand-user? If yo...       0  \n",
       "2679  @TinyJecht Are you another Stand-user? If you ...       0  \n",
       "2828  #KCA #VoteJKT48ID 12News: UPDATE: A family of ...       1  \n",
       "2831  .POTUS #StrategicPatience is a strategy for #G...       1  \n",
       "2832  .POTUS #StrategicPatience is a strategy for #G...       0  \n",
       "2833  .POTUS #StrategicPatience is a strategy for #G...       1  \n",
       "2841  #Myanmar  Displaced #Rohingya at #Sittwe point...       1  \n",
       "3243  He came to a land which was engulfed in tribal...       1  \n",
       "3248  He came to a land which was engulfed in tribal...       1  \n",
       "3251  He came to a land which was engulfed in tribal...       0  \n",
       "3261  He came to a land which was engulfed in tribal...       0  \n",
       "3266  He came to a land which was engulfed in tribal...       0  \n",
       "3374  Elem Pomo helping the displaced from the Rocky...       1  \n",
       "3382  @batfanuk we enjoyed the show today. Great fun...       0  \n",
       "3390  New evacuation ordered for 25 homes in danger ...       1  \n",
       "3461  that exploded &amp; brought about the\\r\\nbegin...       0  \n",
       "3589  11-Year-Old Boy Charged With Manslaughter of T...       1  \n",
       "3591  11-Year-Old Boy Charged With Manslaughter of T...       1  \n",
       "3597  11-Year-Old Boy Charged With Manslaughter of T...       1  \n",
       "3600  11-Year-Old Boy Charged With Manslaughter of T...       1  \n",
       "3603  11-Year-Old Boy Charged With Manslaughter of T...       1  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[df['text'].duplicated()].head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab89c47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "075ddd45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b6b584a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd1f935",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "213589d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "05eeb474",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['id','keyword','location'],axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "97475d28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  target\n",
       "0  Our Deeds are the Reason of this #earthquake M...       1\n",
       "1             Forest fire near La Ronge Sask. Canada       1\n",
       "2  All residents asked to 'shelter in place' are ...       1\n",
       "3  13,000 people receive #wildfires evacuation or...       1\n",
       "4  Just got sent this photo from Ruby #Alaska as ...       1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "15219185",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessor(dataframe):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7ef1cd2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_pos(treebank_tag):\n",
    "\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return ''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a9391f4",
   "metadata": {},
   "source": [
    "# Tweet Tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d678ab12",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweeter = TweetTokenizer(strip_handles=True,reduce_len=True, preserve_case=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fc2f41f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tweets'] = df['text'].apply(lambda text: tweeter.tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "50ee6607",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       [our, deeds, are, the, reason, of, this, #eart...\n",
       "1        [forest, fire, near, la, ronge, sask, ., canada]\n",
       "2       [all, residents, asked, to, ', shelter, in, pl...\n",
       "3       [13,000, people, receive, #wildfires, evacuati...\n",
       "4       [just, got, sent, this, photo, from, ruby, #al...\n",
       "                              ...                        \n",
       "7608    [two, giant, cranes, holding, a, bridge, colla...\n",
       "7609    [the, out, of, control, wild, fires, in, calif...\n",
       "7610    [m1, ., 94, [, 01:04, utc, ], ?, 5km, s, of, v...\n",
       "7611    [police, investigating, after, an, e-bike, col...\n",
       "7612    [the, latest, :, more, homes, razed, by, north...\n",
       "Name: tweets, Length: 7613, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['tweets']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "40bb83e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemma = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fb79eb11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatizing(sentence_array):\n",
    "    lemmas = nltk.pos_tag(sentence_array)\n",
    "    container = []\n",
    "    for word,lem in lemmas:\n",
    "        pos = get_wordnet_pos(lem)\n",
    "        if pos:\n",
    "            container.append(lemma.lemmatize(word,pos=pos))\n",
    "        else:\n",
    "            container.append(lemma.lemmatize(word))\n",
    "    return container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ec6c5038",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['lemmed'] = df['tweets'].apply(lambda array: lemmatizing(array))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "639aab04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       [our, deed, be, the, reason, of, this, #earthq...\n",
       "1        [forest, fire, near, la, ronge, sask, ., canada]\n",
       "2       [all, resident, ask, to, ', shelter, in, place...\n",
       "3       [13,000, people, receive, #wildfires, evacuati...\n",
       "4       [just, get, send, this, photo, from, ruby, #al...\n",
       "                              ...                        \n",
       "7608    [two, giant, crane, hold, a, bridge, collapse,...\n",
       "7609    [the, out, of, control, wild, fire, in, califo...\n",
       "7610    [m1, ., 94, [, 01:04, utc, ], ?, 5km, s, of, v...\n",
       "7611    [police, investigate, after, an, e-bike, colli...\n",
       "7612    [the, late, :, more, home, raze, by, northern,...\n",
       "Name: lemmed, Length: 7613, dtype: object"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['lemmed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b896cc2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaner(sentence_array):\n",
    "    sentence = \" \".join(sentence_array)\n",
    "    sentence = re.sub(r\"http\\S+\", \"\", sentence)\n",
    "    sentence = re.sub('[()!?]', ' ', sentence)\n",
    "    sentence = re.sub('[()!?]', ' ', sentence)\n",
    "    sentence = re.sub('\\[.*?\\]',' ', sentence)\n",
    "    sentence = re.sub(\"[^a-z0-9]\",\" \", sentence)\n",
    "    sentence = re.sub(r'\\b\\w{1,3}\\b', '', sentence)\n",
    "    sentence = [w for w in sentence.split() if not w in stopwords.words('english')]\n",
    "\n",
    "    return \" \".join(sentence)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "17f7ae0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['cleaned'] = df['lemmed'].apply(lambda array: cleaner(array))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "601a752f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                    deed reason earthquake allah forgive\n",
       "1                      forest fire near ronge sask canada\n",
       "2       resident shelter place notify officer evacuati...\n",
       "3       people receive wildfires evacuation order cali...\n",
       "4       send photo ruby alaska smoke wildfires pour sc...\n",
       "                              ...                        \n",
       "7608         giant crane hold bridge collapse nearby home\n",
       "7609    control wild fire california even northern par...\n",
       "7610                                       volcano hawaii\n",
       "7611    police investigate bike collided little portug...\n",
       "7612     late home raze northern california wildfire news\n",
       "Name: cleaned, Length: 7613, dtype: object"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['cleaned']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e193f330",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['cleaned'].isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "386b3ea2",
   "metadata": {},
   "source": [
    "# Machine learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "31065006",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#reminder to build preprocessor function and insert in the vectorizers instead of using the default\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "#XGBClassifier\n",
    "#Decision Trees\n",
    "#Reg NN\n",
    "#LSTM\n",
    "\n",
    "from keras import models, layers, optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "05eb3b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df['cleaned'],df['target'],test_size = .2, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "467a41bf",
   "metadata": {},
   "source": [
    "## CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "225a3b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0c0ab0a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "counted_vect = counter.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "95637951",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counted_vect.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8c15ac7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "counted_df = pd.DataFrame(data=counted_vect.todense(),columns=counter.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f94e31d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>00pm</th>\n",
       "      <th>066gp</th>\n",
       "      <th>06jst</th>\n",
       "      <th>0853</th>\n",
       "      <th>08780923344</th>\n",
       "      <th>0880</th>\n",
       "      <th>0fsloths</th>\n",
       "      <th>0npzp</th>\n",
       "      <th>0sed</th>\n",
       "      <th>1000</th>\n",
       "      <th>...</th>\n",
       "      <th>zipper</th>\n",
       "      <th>zippoline</th>\n",
       "      <th>zl1ns2i</th>\n",
       "      <th>zmne</th>\n",
       "      <th>zombie</th>\n",
       "      <th>zombiefunrun2014</th>\n",
       "      <th>zone</th>\n",
       "      <th>zonewolf</th>\n",
       "      <th>zouma</th>\n",
       "      <th>zumiez</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6085</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6086</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6087</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6088</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6089</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6090 rows × 9677 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      00pm  066gp  06jst  0853  08780923344  0880  0fsloths  0npzp  0sed  \\\n",
       "0        0      0      0     0            0     0         0      0     0   \n",
       "1        0      0      0     0            0     0         0      0     0   \n",
       "2        0      0      0     0            0     0         0      0     0   \n",
       "3        0      0      0     0            0     0         0      0     0   \n",
       "4        0      0      0     0            0     0         0      0     0   \n",
       "...    ...    ...    ...   ...          ...   ...       ...    ...   ...   \n",
       "6085     0      0      0     0            0     0         0      0     0   \n",
       "6086     0      0      0     0            0     0         0      0     0   \n",
       "6087     0      0      0     0            0     0         0      0     0   \n",
       "6088     0      0      0     0            0     0         0      0     0   \n",
       "6089     0      0      0     0            0     0         0      0     0   \n",
       "\n",
       "      1000  ...  zipper  zippoline  zl1ns2i  zmne  zombie  zombiefunrun2014  \\\n",
       "0        0  ...       0          0        0     0       0                 0   \n",
       "1        0  ...       0          0        0     0       0                 0   \n",
       "2        0  ...       0          0        0     0       0                 0   \n",
       "3        0  ...       0          0        0     0       0                 0   \n",
       "4        0  ...       0          0        0     0       0                 0   \n",
       "...    ...  ...     ...        ...      ...   ...     ...               ...   \n",
       "6085     0  ...       0          0        0     0       0                 0   \n",
       "6086     0  ...       0          0        0     0       0                 0   \n",
       "6087     0  ...       0          0        0     0       0                 0   \n",
       "6088     0  ...       0          0        0     0       0                 0   \n",
       "6089     0  ...       0          0        0     0       0                 0   \n",
       "\n",
       "      zone  zonewolf  zouma  zumiez  \n",
       "0        0         0      0       0  \n",
       "1        0         0      0       0  \n",
       "2        0         0      0       0  \n",
       "3        0         0      0       0  \n",
       "4        0         0      0       0  \n",
       "...    ...       ...    ...     ...  \n",
       "6085     0         0      0       0  \n",
       "6086     0         0      0       0  \n",
       "6087     0         0      0       0  \n",
       "6088     0         0      0       0  \n",
       "6089     0         0      0       0  \n",
       "\n",
       "[6090 rows x 9677 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counted_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "578ff283",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_c = counter.transform(X_train)\n",
    "X_test_c = counter.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "85ff24ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log = LogisticRegression()\n",
    "log.fit(X_train_c,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "03162d84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.788575180564675"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log.score(X_test_c,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a4cb9d1",
   "metadata": {},
   "source": [
    "## TFidf Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "80082b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer()\n",
    "tf_vect = tfidf.fit_transform(X_train)\n",
    "X_train_tf = tfidf.transform(X_train)\n",
    "X_test_tf = tfidf.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9570873e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logtf = LogisticRegression()\n",
    "logtf.fit(X_train_tf,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8214319f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7898883782009193"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logtf.score(X_test_tf,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a329b2a",
   "metadata": {},
   "source": [
    "### TFIDF part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ebbd61be",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf2 = TfidfVectorizer()\n",
    "tf_vect2 = tfidf2.fit_transform(X_train)\n",
    "X_train_tf2 = tfidf2.transform(X_train)\n",
    "X_test_tf2 = tfidf2.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b61b7e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe2234c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1cdb9cdd",
   "metadata": {},
   "source": [
    "# Machine Learning Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ca61b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "636fde5e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b774fa89",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be7ca27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79683a1e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47986a13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8277687e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2b6f7e7c",
   "metadata": {},
   "source": [
    "# Importing Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bfe736c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(r'nlp-getting-started/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "43c9339d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just happened a terrible car crash</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Heard about #earthquake is different cities, s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>there is a forest fire at spot pond, geese are...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Apocalypse lighting. #Spokane #wildfires</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Typhoon Soudelor kills 28 in China and Taiwan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3258</th>\n",
       "      <td>10861</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>EARTHQUAKE SAFETY LOS ANGELES ÛÒ SAFETY FASTE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3259</th>\n",
       "      <td>10865</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Storm in RI worse than last hurricane. My city...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3260</th>\n",
       "      <td>10868</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Green Line derailment in Chicago http://t.co/U...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3261</th>\n",
       "      <td>10874</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MEG issues Hazardous Weather Outlook (HWO) htt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3262</th>\n",
       "      <td>10875</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>#CityofCalgary has activated its Municipal Eme...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3263 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id keyword location  \\\n",
       "0         0     NaN      NaN   \n",
       "1         2     NaN      NaN   \n",
       "2         3     NaN      NaN   \n",
       "3         9     NaN      NaN   \n",
       "4        11     NaN      NaN   \n",
       "...     ...     ...      ...   \n",
       "3258  10861     NaN      NaN   \n",
       "3259  10865     NaN      NaN   \n",
       "3260  10868     NaN      NaN   \n",
       "3261  10874     NaN      NaN   \n",
       "3262  10875     NaN      NaN   \n",
       "\n",
       "                                                   text  \n",
       "0                    Just happened a terrible car crash  \n",
       "1     Heard about #earthquake is different cities, s...  \n",
       "2     there is a forest fire at spot pond, geese are...  \n",
       "3              Apocalypse lighting. #Spokane #wildfires  \n",
       "4         Typhoon Soudelor kills 28 in China and Taiwan  \n",
       "...                                                 ...  \n",
       "3258  EARTHQUAKE SAFETY LOS ANGELES ÛÒ SAFETY FASTE...  \n",
       "3259  Storm in RI worse than last hurricane. My city...  \n",
       "3260  Green Line derailment in Chicago http://t.co/U...  \n",
       "3261  MEG issues Hazardous Weather Outlook (HWO) htt...  \n",
       "3262  #CityofCalgary has activated its Municipal Eme...  \n",
       "\n",
       "[3263 rows x 4 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "695b973f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "307fd58f",
   "metadata": {},
   "source": [
    "# answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bd4c279a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>tweets</th>\n",
       "      <th>lemmed</th>\n",
       "      <th>cleaned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "      <td>[our, deeds, are, the, reason, of, this, #eart...</td>\n",
       "      <td>[our, deed, be, the, reason, of, this, #earthq...</td>\n",
       "      <td>deed reason earthquake allah forgive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "      <td>[forest, fire, near, la, ronge, sask, ., canada]</td>\n",
       "      <td>[forest, fire, near, la, ronge, sask, ., canada]</td>\n",
       "      <td>forest fire near ronge sask canada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[all, residents, asked, to, ', shelter, in, pl...</td>\n",
       "      <td>[all, resident, ask, to, ', shelter, in, place...</td>\n",
       "      <td>resident shelter place notify officer evacuati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "      <td>[13,000, people, receive, #wildfires, evacuati...</td>\n",
       "      <td>[13,000, people, receive, #wildfires, evacuati...</td>\n",
       "      <td>people receive wildfires evacuation order cali...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[just, got, sent, this, photo, from, ruby, #al...</td>\n",
       "      <td>[just, get, send, this, photo, from, ruby, #al...</td>\n",
       "      <td>send photo ruby alaska smoke wildfires pour sc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
<<<<<<< HEAD
       "      <th>6075</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6076</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6077</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6078</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6079</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6080 rows × 8076 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      aaaaaaallll  aaarrrgghhh  ab  aba  abandon  abbott  abbswinston  abe  \\\n",
       "0               0            0   0    0        0       0            0    0   \n",
       "1               0            0   0    0        0       0            0    0   \n",
       "2               0            0   0    0        0       0            0    0   \n",
       "3               0            0   0    0        0       0            0    0   \n",
       "4               0            0   0    0        0       0            0    0   \n",
       "...           ...          ...  ..  ...      ...     ...          ...  ...   \n",
       "6075            0            0   0    0        0       0            0    0   \n",
       "6076            0            0   0    0        0       0            0    0   \n",
       "6077            0            0   0    0        0       0            0    0   \n",
       "6078            0            0   0    0        0       0            0    0   \n",
       "6079            0            0   0    0        0       0            0    0   \n",
       "\n",
       "      aberdeen  ability  ...  zionist  zionists  zip  zipped  zipper  zombie  \\\n",
       "0            0        0  ...        0         0    0       0       0       0   \n",
       "1            0        0  ...        0         0    0       0       0       0   \n",
       "2            0        0  ...        0         0    0       0       0       0   \n",
       "3            0        0  ...        0         0    0       0       0       0   \n",
       "4            0        0  ...        0         0    0       0       0       0   \n",
       "...        ...      ...  ...      ...       ...  ...     ...     ...     ...   \n",
       "6075         0        0  ...        0         0    0       0       0       0   \n",
       "6076         0        0  ...        0         0    0       0       0       0   \n",
       "6077         0        0  ...        0         0    0       0       0       0   \n",
       "6078         0        0  ...        0         0    0       0       0       0   \n",
       "6079         0        0  ...        0         0    0       0       0       0   \n",
       "\n",
       "      zone  zoom  zurich  zzz  \n",
       "0        0     0       0    0  \n",
       "1        0     0       0    0  \n",
       "2        0     0       0    0  \n",
       "3        0     0       0    0  \n",
       "4        0     0       0    0  \n",
       "...    ...   ...     ...  ...  \n",
       "6075     0     0       0    0  \n",
       "6076     0     0       0    0  \n",
       "6077     0     0       0    0  \n",
       "6078     0     0       0    0  \n",
       "6079     0     0       0    0  \n",
       "\n",
       "[6080 rows x 8076 columns]"
      ]
     },
     "execution_count": 32,
=======
       "      <th>7608</th>\n",
       "      <td>Two giant cranes holding a bridge collapse int...</td>\n",
       "      <td>1</td>\n",
       "      <td>[two, giant, cranes, holding, a, bridge, colla...</td>\n",
       "      <td>[two, giant, crane, hold, a, bridge, collapse,...</td>\n",
       "      <td>giant crane hold bridge collapse nearby home</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7609</th>\n",
       "      <td>@aria_ahrary @TheTawniest The out of control w...</td>\n",
       "      <td>1</td>\n",
       "      <td>[the, out, of, control, wild, fires, in, calif...</td>\n",
       "      <td>[the, out, of, control, wild, fire, in, califo...</td>\n",
       "      <td>control wild fire california even northern par...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7610</th>\n",
       "      <td>M1.94 [01:04 UTC]?5km S of Volcano Hawaii. htt...</td>\n",
       "      <td>1</td>\n",
       "      <td>[m1, ., 94, [, 01:04, utc, ], ?, 5km, s, of, v...</td>\n",
       "      <td>[m1, ., 94, [, 01:04, utc, ], ?, 5km, s, of, v...</td>\n",
       "      <td>volcano hawaii</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7611</th>\n",
       "      <td>Police investigating after an e-bike collided ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[police, investigating, after, an, e-bike, col...</td>\n",
       "      <td>[police, investigate, after, an, e-bike, colli...</td>\n",
       "      <td>police investigate bike collided little portug...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7612</th>\n",
       "      <td>The Latest: More Homes Razed by Northern Calif...</td>\n",
       "      <td>1</td>\n",
       "      <td>[the, latest, :, more, homes, razed, by, north...</td>\n",
       "      <td>[the, late, :, more, home, raze, by, northern,...</td>\n",
       "      <td>late home raze northern california wildfire news</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7613 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  target  \\\n",
       "0     Our Deeds are the Reason of this #earthquake M...       1   \n",
       "1                Forest fire near La Ronge Sask. Canada       1   \n",
       "2     All residents asked to 'shelter in place' are ...       1   \n",
       "3     13,000 people receive #wildfires evacuation or...       1   \n",
       "4     Just got sent this photo from Ruby #Alaska as ...       1   \n",
       "...                                                 ...     ...   \n",
       "7608  Two giant cranes holding a bridge collapse int...       1   \n",
       "7609  @aria_ahrary @TheTawniest The out of control w...       1   \n",
       "7610  M1.94 [01:04 UTC]?5km S of Volcano Hawaii. htt...       1   \n",
       "7611  Police investigating after an e-bike collided ...       1   \n",
       "7612  The Latest: More Homes Razed by Northern Calif...       1   \n",
       "\n",
       "                                                 tweets  \\\n",
       "0     [our, deeds, are, the, reason, of, this, #eart...   \n",
       "1      [forest, fire, near, la, ronge, sask, ., canada]   \n",
       "2     [all, residents, asked, to, ', shelter, in, pl...   \n",
       "3     [13,000, people, receive, #wildfires, evacuati...   \n",
       "4     [just, got, sent, this, photo, from, ruby, #al...   \n",
       "...                                                 ...   \n",
       "7608  [two, giant, cranes, holding, a, bridge, colla...   \n",
       "7609  [the, out, of, control, wild, fires, in, calif...   \n",
       "7610  [m1, ., 94, [, 01:04, utc, ], ?, 5km, s, of, v...   \n",
       "7611  [police, investigating, after, an, e-bike, col...   \n",
       "7612  [the, latest, :, more, homes, razed, by, north...   \n",
       "\n",
       "                                                 lemmed  \\\n",
       "0     [our, deed, be, the, reason, of, this, #earthq...   \n",
       "1      [forest, fire, near, la, ronge, sask, ., canada]   \n",
       "2     [all, resident, ask, to, ', shelter, in, place...   \n",
       "3     [13,000, people, receive, #wildfires, evacuati...   \n",
       "4     [just, get, send, this, photo, from, ruby, #al...   \n",
       "...                                                 ...   \n",
       "7608  [two, giant, crane, hold, a, bridge, collapse,...   \n",
       "7609  [the, out, of, control, wild, fire, in, califo...   \n",
       "7610  [m1, ., 94, [, 01:04, utc, ], ?, 5km, s, of, v...   \n",
       "7611  [police, investigate, after, an, e-bike, colli...   \n",
       "7612  [the, late, :, more, home, raze, by, northern,...   \n",
       "\n",
       "                                                cleaned  \n",
       "0                  deed reason earthquake allah forgive  \n",
       "1                    forest fire near ronge sask canada  \n",
       "2     resident shelter place notify officer evacuati...  \n",
       "3     people receive wildfires evacuation order cali...  \n",
       "4     send photo ruby alaska smoke wildfires pour sc...  \n",
       "...                                                 ...  \n",
       "7608       giant crane hold bridge collapse nearby home  \n",
       "7609  control wild fire california even northern par...  \n",
       "7610                                     volcano hawaii  \n",
       "7611  police investigate bike collided little portug...  \n",
       "7612   late home raze northern california wildfire news  \n",
       "\n",
       "[7613 rows x 5 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc8e4f1e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "2e16d9f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_vect = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "be254b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc1 = \"This is a dog\"\n",
    "doc2 = \"This is a cat\"\n",
    "doc3 = \"The cat and the dog dont like the other cat\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "c7439446",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df = pd.DataFrame([doc1,doc2,doc3],columns=[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "2fc459a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This is a dog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>This is a cat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The cat and the dog dont like the other cat</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          text\n",
       "0                                This is a dog\n",
       "1                                This is a cat\n",
       "2  The cat and the dog dont like the other cat"
      ]
     },
     "execution_count": 73,
>>>>>>> ng_branch
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_df"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 33,
   "id": "578ff283",
=======
   "execution_count": 74,
   "id": "bee3bc2b",
>>>>>>> ng_branch
   "metadata": {},
   "outputs": [],
   "source": [
    "vecc = sample_vect.fit_transform(sample_df['text'])"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 34,
   "id": "85ff24ff",
=======
   "execution_count": 75,
   "id": "c55f02d6",
>>>>>>> ng_branch
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0, 0, 1, 0, 1, 0, 0, 0, 1],\n",
       "        [0, 1, 0, 0, 1, 0, 0, 0, 1],\n",
       "        [1, 2, 1, 1, 0, 1, 1, 3, 0]], dtype=int64)"
      ]
     },
<<<<<<< HEAD
     "execution_count": 34,
=======
     "execution_count": 75,
>>>>>>> ng_branch
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vecc.todense()"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 35,
   "id": "03162d84",
=======
   "execution_count": 76,
   "id": "bbafdd00",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = pd.DataFrame(vecc.todense(), columns = sample_vect.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "78c6b5f0",
>>>>>>> ng_branch
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>and</th>\n",
       "      <th>cat</th>\n",
       "      <th>dog</th>\n",
       "      <th>dont</th>\n",
       "      <th>is</th>\n",
       "      <th>like</th>\n",
       "      <th>other</th>\n",
       "      <th>the</th>\n",
       "      <th>this</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
<<<<<<< HEAD
       "0.7928994082840237"
      ]
     },
     "execution_count": 35,
=======
       "   and  cat  dog  dont  is  like  other  the  this\n",
       "0    0    0    1     0   1     0      0    0     1\n",
       "1    0    1    0     0   1     0      0    0     1\n",
       "2    1    2    1     1   0     1      1    3     0"
      ]
     },
     "execution_count": 77,
>>>>>>> ng_branch
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
<<<<<<< HEAD
    "log.score(X_test_c,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a4cb9d1",
   "metadata": {},
   "source": [
    "## TFidf Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "80082b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer()\n",
    "tf_vect = tfidf.fit_transform(X_train)\n",
    "X_train_tf = tfidf.transform(X_train)\n",
    "X_test_tf = tfidf.transform(X_test)"
=======
    "output"
>>>>>>> ng_branch
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 37,
   "id": "9570873e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
=======
   "execution_count": 78,
   "id": "d07ff69d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a dog\n",
      "This is a cat\n",
      "The cat and the dog dont like the other cat\n"
     ]
>>>>>>> ng_branch
    }
   ],
   "source": [
    "print(doc1)\n",
    "print(doc2)\n",
    "print(doc3)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 38,
   "id": "8214319f",
=======
   "execution_count": 84,
   "id": "0019e6ce",
>>>>>>> ng_branch
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cat</th>\n",
       "      <th>dog</th>\n",
       "      <th>is</th>\n",
       "      <th>other cat</th>\n",
       "      <th>the</th>\n",
       "      <th>the cat</th>\n",
       "      <th>the dog</th>\n",
       "      <th>the other</th>\n",
       "      <th>this</th>\n",
       "      <th>this is</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.381553</td>\n",
       "      <td>0.190776</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.250848</td>\n",
       "      <td>0.752544</td>\n",
       "      <td>0.250848</td>\n",
       "      <td>0.250848</td>\n",
       "      <td>0.250848</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
<<<<<<< HEAD
       "0.8034188034188035"
      ]
     },
     "execution_count": 38,
=======
       "        cat       dog   is  other cat       the   the cat   the dog  \\\n",
       "0  0.000000  0.500000  0.5   0.000000  0.000000  0.000000  0.000000   \n",
       "1  0.500000  0.000000  0.5   0.000000  0.000000  0.000000  0.000000   \n",
       "2  0.381553  0.190776  0.0   0.250848  0.752544  0.250848  0.250848   \n",
       "\n",
       "   the other  this  this is  \n",
       "0   0.000000   0.5      0.5  \n",
       "1   0.000000   0.5      0.5  \n",
       "2   0.250848   0.0      0.0  "
      ]
     },
     "execution_count": 84,
>>>>>>> ng_branch
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samptf = TfidfVectorizer(ngram_range=[1,2],max_features=10)\n",
    "fit_tf = samptf.fit_transform(sample_df['text'])\n",
    "tf_df = pd.DataFrame(fit_tf.todense(),columns=samptf.get_feature_names())\n",
    "tf_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "152a219b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a dog\n",
      "This is a cat\n",
      "The cat and the dog dont like the other cat\n"
     ]
    }
   ],
   "source": [
    "print(doc1)\n",
    "print(doc2)\n",
    "print(doc3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
<<<<<<< HEAD
   "id": "0b61b7e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "28868990",
   "metadata": {},
   "source": [
    "# Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8a8ebc0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_to_dataset(dataframe, shuffle=True, batch_size=32):\n",
    "    df = dataframe.copy()\n",
    "    labels = df.pop('target')\n",
    "    df = df['text']\n",
    "    ds = tf.data.Dataset.from_tensor_slices((df, labels))\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(buffer_size=len(dataframe))\n",
    "    ds = ds.batch(batch_size)\n",
    "    ds = ds.prefetch(tf.data.AUTOTUNE)\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bfe736c6",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\NEL-JI~1\\AppData\\Local\\Temp/ipykernel_7076/391604064.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c9339d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "695b973f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac52f5a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a207e4",
=======
   "id": "f9487c25",
>>>>>>> ng_branch
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bccb2c88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af5c013",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4bfb659",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb511e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b98393",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccef3d42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c2419bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ea5265",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "aab6a942",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['m1',\n",
       " '.',\n",
       " '94',\n",
       " '[',\n",
       " '01:04',\n",
       " 'utc',\n",
       " ']',\n",
       " '?',\n",
       " '5km',\n",
       " 's',\n",
       " 'of',\n",
       " 'volcano',\n",
       " 'hawaii',\n",
       " '.',\n",
       " 'http://t.co/zDtoyd8EbJ']"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "d0126836",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['volcano', 'hawaii']"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaner(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "047ad4bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = df['lemmed'].iloc[7610]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "5ad005be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['m1',\n",
       " '.',\n",
       " '94',\n",
       " '[',\n",
       " '01:04',\n",
       " 'utc',\n",
       " ']',\n",
       " '?',\n",
       " '5km',\n",
       " 's',\n",
       " 'of',\n",
       " 'volcano',\n",
       " 'hawaii',\n",
       " '.',\n",
       " 'http://t.co/zDtoyd8EbJ']"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "41052b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_temp = re.sub(r\"http\\S+\", \"\", \" \".join(temp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "b28fbb14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'m1 . 94 [ 01:04 utc ] ? 5km s of volcano hawaii . '"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "46416258",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_temp2 = re.sub('[()!?]', ' ', new_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "ffec703b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'m1 . 94 [ 01:04 utc ]   5km s of volcano hawaii . '"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_temp2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "d300b4e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_temp3 = re.sub('\\[.*?\\]',' ', new_temp2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "803ce4fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'m1 . 94     5km s of volcano hawaii . '"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_temp3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "43565f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_temp4 = re.sub(\"[^a-z0-9]\",\" \", new_temp3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "2fda8d99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'m1   94     5km s of volcano hawaii   '"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_temp4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "00550475",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_stopwords = [\"for\", \"on\", \"an\", \"a\", \"of\", \"and\", \"in\", \"the\", \"to\", \"from\"]\n",
    "\n",
    "temp5 = [w for w in new_temp4.split() if not w in test_stopwords]\n",
    "temp6 = \" \".join(word for word in temp5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "b6bec9a3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'m1 94 5km s volcano hawaii'"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "6e6095ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp7 = re.sub(r'\\b\\w{1,3}\\b', '', temp6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "e7d08bda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['volcano', 'hawaii']"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp7.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "01d0c2c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "3c6ef193",
   "metadata": {},
   "outputs": [],
   "source": [
    "container2 = []\n",
    "for word in stopwords.words('english'):\n",
    "    if len(word)==1:\n",
    "        container2.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "8871879d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'a', 's', 't', 'd', 'm', 'o', 'y']"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "container2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec5919b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a62044d5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dog'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemma.lemmatize(\"dogs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "86b4c50e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'go'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemma.lemmatize(\"going\",pos = \"v\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "499920b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dog\n",
      "going\n",
      "to\n",
      "dancing\n"
     ]
    }
   ],
   "source": [
    "for word in \"dogs going to dancing\".split():\n",
    "    print(lemma.lemmatize(word))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "61414b46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('dog', 'NN')]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.pos_tag(['dog'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bf16552b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'nltk' has no attribute 'pos'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\NEL-JI~1\\AppData\\Local\\Temp/ipykernel_20016/3096961139.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpos\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'going'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: module 'nltk' has no attribute 'pos'"
     ]
    }
   ],
   "source": [
    "nltk.pos(['going'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "6f226ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"dogs going to be playing\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "52dccc13",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_lemmas = lemma.lemmatize(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "9494d148",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_lemmas = nltk.pos_tag(sentence.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "cc94ff21",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('dogs', 'NNS'),\n",
       " ('going', 'VBG'),\n",
       " ('to', 'TO'),\n",
       " ('be', 'VB'),\n",
       " ('playing', 'VBG')]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "54688b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "container = []\n",
    "for word,lem in test_lemmas:\n",
    "    pos = get_wordnet_pos(lem)\n",
    "    if pos:\n",
    "        container.append(lemma.lemmatize(word,pos=pos))\n",
    "    else:\n",
    "        container.append(lemma.lemmatize(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "576c82a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dog', 'go', 'to', 'be', 'play']"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "fd0bcd97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'n'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_wordnet_pos(nltk.pos_tag(['dog'])[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f48f62fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "tag = nltk.pos_tag(['going'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "49753076",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'v'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_wordnet_pos(tag[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c2e17393",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('going', 'VBG')]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c3bbea3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "195.067px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
